{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/NBM_4_1_Reliability_With_Obs%2BURMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# INSTALL AND IMPORTS                                                         #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "!pip install boto3\n",
        "!pip install pygrib\n",
        "!pip install swifter\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import boto3\n",
        "import pygrib\n",
        "import swifter\n",
        "import zipfile\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "from google.colab import files\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from multiprocessing import set_start_method, get_context\n",
        "\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.metrics import (brier_score_loss, f1_score, log_loss,\n",
        "                                precision_score, recall_score, roc_auc_score)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# COLAB MARKDOWN AND USER CONFIGS                                             #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# @markdown <FONT SIZE=5>**1. Please Provide Your Synoptic API Token...**\n",
        "user_token = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**2. Select Start and End Dates**\n",
        "# @markdown <br><FONT SIZE=3>NBM 4.1 and 4.2 available as a threaded dataset\n",
        "# @markdown for PMaxT, PMinT, PQPF from 1/18/2023 to present\n",
        "start_date = \"2023-06-01\" # @param {type:\"date\"}\n",
        "end_date = \"2023-09-01\" # @param {type:\"date\"}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**3. For Which Element?**\n",
        "element = \"maxt\" # @param [\"maxt\", \"mint\", \"qpf24\", \"qpf12\", \"qpf06\"]\n",
        "\n",
        "# Split element/interval\n",
        "interval_selection = int(element[-2:]) if \"qpf\" in element else False\n",
        "element = element[:3] if \"qpf\" in element else element\n",
        "\n",
        "#6/12/24/48/72, if element==temp then False\n",
        "# interval_selection = \"24\" #@param [\"24\", \"12\", \"6\"]\n",
        "# interval_selection = interval_selection if element == \"qpf\" else False\n",
        "\n",
        "#temperature_threshold = -60 #@param {type:\"slider\", min:-60, max:140, step:10}\n",
        "#qpf_threshold = 0.31 #@param {type:\"slider\", min:0.01, max:5.00, step:0.01}\n",
        "\n",
        "#if element in [\"maxt\",\"mint\"]:\n",
        "#    threshold = temperature_threshold\n",
        "#elif element in [\"qpf\"]:\n",
        "#    threshold = qpf_threshold\n",
        "\n",
        "# @markdown <FONT SIZE=5>**4. For Which Lead Time (in days)?**\n",
        "lead_days_selection = 3 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**5. For Which Region?**\n",
        "region_selection = \"CWA\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"CONUS\", \"CWA\", \"RFC\"]\n",
        "\n",
        "#@markdown If CWA/RFC selected, which one? (i.e. \"SLC\" for Salt Lake City, \"CBRFC\" for Colorado Basin)\n",
        "cwa_selection = '' #@param {type:\"string\"}\n",
        "\n",
        "# @markdown For Which Networks?\n",
        "network_selection = 'NWS+RAWS+HADS' #@param [\"NWS+RAWS\", \"NWS+RAWS+HADS\", \"NWS\", \"RAWS\", \"HADS\", \"SNOTEL\", \"ALL\", \"CUSTOM\"]\n",
        "\n",
        "#@markdown Enter comma separated network IDs (custom) or siteids (list)  WITH NO SPACES here. For help - https://developers.synopticdata.com/about/station-providers/\n",
        "network_input = '' #@param {type:\"string\"}\n",
        "\n",
        "# @markdown Check box to display plots inline (default unchecked for download)\n",
        "display_inline = False #@param {type:\"boolean\"}\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# GLOBAL VARIABLES AND GENERAL CONFIG                                         #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Multiprocess settings\n",
        "process_pool_size = 20 #cpu_count()*16\n",
        "print(f'Process Pool Size: {process_pool_size}')\n",
        "\n",
        "# Backend APIs\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "qc_api = \"https://api.synopticdata.com/v2/stations/qcsegments?\"\n",
        "\n",
        "# Data Query APIs\n",
        "timeseries_api = \"https://api.synopticdata.com/v2/stations/timeseries?\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "\n",
        "# Assign API to element name\n",
        "synoptic_apis = {\n",
        "    'qpf':precipitation_api,\n",
        "    'maxt':statistics_api,\n",
        "    'mint':statistics_api}\n",
        "\n",
        "synoptic_networks = {\"NWS+RAWS+HADS\":\"1,2,106\",\n",
        "                     \"NWS+RAWS\":\"1,2\",\n",
        "                     \"NWS\":\"1\",\n",
        "                     \"RAWS\": \"2\",\n",
        "                     \"HADS\": \"106\",\n",
        "                     \"SNOTEL\":\"25\",\n",
        "                     \"ALL\":None,\n",
        "                     \"CUSTOM\":network_input,}\n",
        "                    #  \"LIST\": \"&stid=\"+network_input}\n",
        "\n",
        "# Assign synoptic variable to element name\n",
        "synoptic_vars = {\n",
        "    'qpf':None,\n",
        "    'maxt':'air_temp',\n",
        "    'mint':'air_temp'}\n",
        "\n",
        "synoptic_vars_out = {\n",
        "    'qpf':'OBSERVATIONS.precipitation',\n",
        "    'maxt':'STATISTICS.air_temp_set_1.maximum',\n",
        "    'mint':'STATISTICS.air_temp_set_1.minimum',}\n",
        "\n",
        "# Assign stat type to element name\n",
        "stat_type = {\n",
        "    'qpf':'interval',\n",
        "    'maxt':'maximum',\n",
        "    'mint':'minimum'}\n",
        "\n",
        "ob_hours = {\n",
        "    'qpf':[['0000', '0000'], ['1200', '1200']],\n",
        "    'maxt':[['1200', '0600']],\n",
        "    'mint':[['0000', '1800']]}\n",
        "\n",
        "# NBM Globals\n",
        "aws_bucket_nbm = 'noaa-nbm-grib2-pds'\n",
        "aws_bucket_urma = 'noaa-urma-pds'\n",
        "\n",
        "# Where to place the grib files (subdirs can be added in local) (not used)\n",
        "output_dir = './' #/nas/stid/data/nbm-verification/temp/'\n",
        "\n",
        "# Which grib variables do each element correlate with\n",
        "nbm_vars = {'qpf':'APCP',\n",
        "                  'maxt':'TMP',\n",
        "                  'mint':'TMP'}\n",
        "\n",
        "# Which grib levels do each element correlate with\n",
        "nbm_levs = {'qpf':'surface',\n",
        "               'maxt':'2 m above ground',\n",
        "               'mint':'2 m above ground'}\n",
        "\n",
        "# If a grib message contains any of these, exclude\n",
        "excludes = ['ens std dev', '% lev']\n",
        "\n",
        "# Fix MDL's kelvin thresholds...\n",
        "tk_fix = {233.0:233.15, 244.0:244.261, 249.0:249.817, 255.0:255.372,\n",
        "    260:260.928, 270.0:270.928, 273.0:273.15, 299.0:299.817,\n",
        "    305.0:305.372, 310.0:310.928, 316.0:316.483, 322.0:322.039}\n",
        "\n",
        "# Convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+' 0000', '%Y-%m-%d %H%M')\n",
        "    for date in [start_date, end_date]]\n",
        "\n",
        "# Bracket start date by 4.1 implementation\n",
        "if start_date < datetime(2023, 1, 18, 0, 0, 0):\n",
        "    start_date = datetime(2023, 1, 18, 0, 0, 0)\n",
        "\n",
        "# Build synoptic arg dict\n",
        "synoptic_api_args = {\n",
        "    'ob_stat':stat_type[element],\n",
        "    'api':synoptic_apis[element],\n",
        "    'element':element,\n",
        "    'interval':interval_selection if element == 'qpf' else False,\n",
        "    'region':region_selection,\n",
        "    'network_query':synoptic_networks[network_selection], # add config feature later\n",
        "    'vars_query':None if element == 'qpf'\n",
        "        else f'{synoptic_vars[element]}',\n",
        "    'days_offset':1 if element != 'mint' else 0}\n",
        "\n",
        "# Build nbm/urma arg dict\n",
        "nbm_request_args = {\n",
        "    'interval':interval_selection if element == 'qpf' else False,\n",
        "    'lead_time_days':lead_days_selection,\n",
        "    'nbm_area':'co',\n",
        "    'element':element,\n",
        "    'var':nbm_vars[element],\n",
        "    'level':nbm_levs[element]}\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# FUNCTIONS AND METHODS (GENERAL)                                             #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def mkdir_p(check_dir):\n",
        "    from pathlib import Path\n",
        "    check_dir = output_dir + check_dir\n",
        "    Path(check_dir).mkdir(parents=True, exist_ok=True)\n",
        "    return check_dir\n",
        "\n",
        "def zip_files(files, zip_name):\n",
        "  \"\"\"\n",
        "  Zips a list of files into a single zipfile.\n",
        "\n",
        "  Args:\n",
        "    files: A list of file paths.\n",
        "    zip_name: The name of the zipfile.\n",
        "  \"\"\"\n",
        "  with zipfile.ZipFile(zip_name, 'w') as zip:\n",
        "    for file in files:\n",
        "      zip.write(file)\n",
        "\n",
        "def cwa_list(input_region):\n",
        "\n",
        "    input_region = input_region.upper()\n",
        "\n",
        "    region_dict ={\n",
        "        \"WR\":[\"BYZ\", \"BOI\", \"LKN\", \"EKA\", \"FGZ\", \"GGW\", \"TFX\", \"VEF\", \"LOX\", \"MFR\",\n",
        "            \"MSO\", \"PDT\", \"PSR\", \"PIH\", \"PQR\", \"REV\", \"STO\", \"SLC\", \"SGX\", \"MTR\",\n",
        "            \"HNX\", \"SEW\", \"OTX\", \"TWC\"],\n",
        "\n",
        "        \"CR\":[\"ABR\", \"BIS\", \"CYS\", \"LOT\", \"DVN\", \"BOU\", \"DMX\", \"DTX\", \"DDC\", \"DLH\",\n",
        "            \"FGF\", \"GLD\", \"GJT\", \"GRR\", \"GRB\", \"GID\", \"IND\", \"JKL\", \"EAX\", \"ARX\",\n",
        "            \"ILX\", \"LMK\", \"MQT\", \"MKX\", \"MPX\", \"LBF\", \"APX\", \"IWX\", \"OAX\", \"PAH\",\n",
        "            \"PUB\", \"UNR\", \"RIW\", \"FSD\", \"SGF\", \"LSX\", \"TOP\", \"ICT\"],\n",
        "\n",
        "        \"ER\":[\"ALY\", \"LWX\", \"BGM\", \"BOX\", \"BUF\", \"BTV\", \"CAR\", \"CTP\", \"RLX\", \"CHS\",\n",
        "            \"ILN\", \"CLE\", \"CAE\", \"GSP\", \"MHX\", \"OKX\", \"PHI\", \"PBZ\", \"GYX\", \"RAH\",\n",
        "            \"RNK\", \"AKQ\", \"ILM\"],\n",
        "\n",
        "        \"SR\":[\"ABQ\", \"AMA\", \"FFC\", \"EWX\", \"BMX\", \"BRO\", \"CRP\", \"EPZ\", \"FWD\", \"HGX\",\n",
        "            \"HUN\", \"JAN\", \"JAX\", \"KEY\", \"MRX\", \"LCH\", \"LZK\", \"LUB\", \"MLB\", \"MEG\",\n",
        "            \"MAF\", \"MFL\", \"MOB\", \"MRX\", \"OHX\", \"LIX\", \"OUN\", \"SJT\", \"SHV\", \"TAE\",\n",
        "            \"TBW\", \"TSA\"]}\n",
        "\n",
        "    if input_region == \"CONUS\":\n",
        "        return np.hstack([region_dict[region] for region in region_dict.keys()])\n",
        "    else:\n",
        "        return region_dict[input_region]\n",
        "\n",
        "def cwa_list_rfc(input_rfc):\n",
        "\n",
        "    metadata_api = 'https://api.synopticdata.com/v2/stations/metadata?'\n",
        "\n",
        "    network_query = (f\"&network={synoptic_networks[network_selection]}\"\n",
        "                    if synoptic_networks[network_selection] is not None else '')\n",
        "\n",
        "    # Assemble the API query\n",
        "    api_query = (f\"{metadata_api}&token={user_token}\" + network_query +\n",
        "                f\"&complete=1&sensorvars=1,obrange=20230118\") #hardcoded for NBM4.1+\n",
        "\n",
        "    # Print the API query to output\n",
        "    # print(api_query)\n",
        "\n",
        "    # Get the data from the API\n",
        "    response = requests.get(api_query)\n",
        "    metadata = pd.DataFrame(response.json()['STATION'])\n",
        "\n",
        "    # Remove NaNs and index by network, station ID\n",
        "    metadata = metadata[metadata['MNET_SHORTNAME'].notna()]\n",
        "    metadata = metadata.set_index(['MNET_SHORTNAME', 'STID'])\n",
        "\n",
        "    metadata['LATITUDE'] = metadata['LATITUDE'].astype(float)\n",
        "    metadata['LONGITUDE'] = metadata['LONGITUDE'].astype(float)\n",
        "    metadata['ELEVATION'] = metadata['ELEVATION'].astype(float)\n",
        "\n",
        "    metadata = metadata[metadata['LATITUDE'] >= 31]\n",
        "    metadata = metadata[metadata['LONGITUDE'] <= -103.00]\n",
        "    metadata = metadata[metadata['STATUS'] == 'ACTIVE']\n",
        "\n",
        "    geometry = gpd.points_from_xy(metadata.LONGITUDE, metadata.LATITUDE)\n",
        "    metadata = gpd.GeoDataFrame(metadata, geometry=geometry)\n",
        "\n",
        "    req = requests.get(\n",
        "        'https://www.weather.gov/source/gis/Shapefiles/Misc/rf05mr24.zip',\n",
        "\n",
        "    allow_redirects=True)\n",
        "    open('rf05mr24.zip', 'wb').write(req.content)\n",
        "\n",
        "    with zipfile.ZipFile('rf05mr24.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    rfc_shp = gpd.read_file('rf05mr24.shp').set_index('BASIN_ID')\n",
        "\n",
        "    metadata = metadata[metadata.geometry.within(rfc_shp.geometry.loc[input_rfc])]\n",
        "\n",
        "    rfc_site_list = metadata.index.get_level_values(1).unique()\n",
        "    rfc_cwa_list = metadata['CWA'].unique()\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# FUNCTIONS AND METHODS (SYNOPTIC API)                                        #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def fetch_obs_from_API(valid_datetime, cwa='', output_type='csv',\n",
        "                       use_saved=True, **req):\n",
        "\n",
        "    if req[\"element\"] == 'qpf':\n",
        "        start_adjusted = (datetime.strptime(valid_datetime, '%Y%m%d%H%M')\n",
        "                          - timedelta(hours=req[\"interval\"]))\n",
        "        end_adjusted = datetime.strptime(valid_datetime, '%Y%m%d%H%M')\n",
        "\n",
        "    elif ((req[\"element\"] == 'maxt') or (req[\"element\"] == 'mint')):\n",
        "        start_adjusted = (datetime.strptime(valid_datetime, '%Y%m%d%H%M')\n",
        "                          - timedelta(hours=18))\n",
        "        end_adjusted = datetime.strptime(valid_datetime, '%Y%m%d%H%M')\n",
        "\n",
        "    valid = True\n",
        "    cwa_filename = (region_selection if region_selection != 'CWA'\n",
        "                    else cwa_selection)\n",
        "\n",
        "    element_label = req['element'] if req['element'] != 'qpf' else \\\n",
        "                        'qpe' + f'{req[\"interval\"]:02d}'\n",
        "\n",
        "\n",
        "    output_file = mkdir_p(f'obs_{output_type}/') +\\\n",
        "        f'obs.{element_label}.{req[\"ob_stat\"]}' +\\\n",
        "        f'.{valid_datetime}.{cwa_filename}.{output_type}'\n",
        "\n",
        "    if os.path.isfile(output_file) & use_saved:\n",
        "        # print(f'Output file exists for:{iter_item}')\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        json_file = mkdir_p('obs_json/') +\\\n",
        "            f'obs.{element_label}.{req[\"ob_stat\"]}' +\\\n",
        "            f'.{valid_datetime}.{cwa_filename}.json'\n",
        "\n",
        "        if os.path.isfile(json_file) & use_saved:\n",
        "            # print(f'Polling archived JSON for: {iter_item}')\n",
        "\n",
        "            with open(json_file, 'rb+') as rfp:\n",
        "                response_dataframe = pd.json_normalize(json.load(rfp)['STATION'])\n",
        "\n",
        "        else:\n",
        "            api_query_args = {\n",
        "                'api_token':f'&token={user_token}',\n",
        "                'station_query':f'&cwa={cwa}',\n",
        "                'network_query':(f'&network={req[\"network_query\"]}'\n",
        "                                 if req[\"network_query\"] is not None else ''),\n",
        "\n",
        "                'start_date_query':f'&start={start_adjusted.strftime(\"%Y%m%d%H%M\")}',\n",
        "                'end_date_query':f'&end={end_adjusted.strftime(\"%Y%m%d%H%M\")}',\n",
        "\n",
        "                'vars_query':(f'&pmode=intervals&interval={req[\"interval\"]}'\n",
        "                              if req[\"element\"] == 'qpf'\n",
        "                                else f'&vars={req[\"vars_query\"]}'),\n",
        "                'stats_query':f'&type={req[\"ob_stat\"]}',\n",
        "                'timezone_query':'&obtimezone=utc',\n",
        "                'api_extras':'&units=temp|f&complete=True'}\n",
        "                    #'&fields=name,status,latitude,longitude,elevation'\n",
        "\n",
        "            api_query = req['api'] + ''.join(\n",
        "                [api_query_args[k] for k in api_query_args.keys()])\n",
        "\n",
        "            print(f'Polling API for: {iter_item}\\n{api_query}')\n",
        "\n",
        "            status_code, response_count = None, 0\n",
        "            while (status_code != 200) & (response_count <= 10):\n",
        "                print(f'{iter_item}, HTTP:{status_code}, #:{response_count}')\n",
        "\n",
        "                # Don't sleep first try, sleep increasing amount for each retry\n",
        "                time.sleep(2*response_count)\n",
        "\n",
        "                response = requests.get(api_query)\n",
        "                # response.raise_for_status()\n",
        "\n",
        "                status_code = response.status_code\n",
        "                response_count += 1\n",
        "\n",
        "            try:\n",
        "                response_dataframe = pd.json_normalize(\n",
        "                    response.json()['STATION'])\n",
        "            except:\n",
        "                valid = False\n",
        "            else:\n",
        "                with open(json_file, 'wb+') as wfp:\n",
        "                    wfp.write(response.content)\n",
        "\n",
        "        if valid:\n",
        "            # Check ACTIVE flag (Can disable in config above if desired)\n",
        "            response_dataframe = response_dataframe[\n",
        "                response_dataframe['STATUS'] == \"ACTIVE\"]\n",
        "\n",
        "            # Un-nest the QPF totals\n",
        "            if req['element'] == 'qpf':\n",
        "                response_dataframe['TOTAL'] = [i[0]['total']\n",
        "                    for i in response_dataframe['OBSERVATIONS.precipitation']]\n",
        "\n",
        "            if output_type == 'pickle':\n",
        "            # Save out df as pickle\n",
        "                response_dataframe.to_pickle(output_file)\n",
        "\n",
        "            elif output_type == 'csv':\n",
        "            # Save out df as csv\n",
        "                response_dataframe.to_csv(output_file)\n",
        "\n",
        "            return None\n",
        "\n",
        "        else:\n",
        "            return iter_item\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# FUNCTIONS AND METHODS (NBM)                                                 #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def ll_to_index(loclat, loclon, datalats, datalons):\n",
        "    # index, loclat, loclon = loclatlon\n",
        "    abslat = np.abs(datalats-loclat)\n",
        "    abslon = np.abs(datalons-loclon)\n",
        "    c = np.maximum(abslon, abslat)\n",
        "    latlon_idx_flat = np.argmin(c)\n",
        "    latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "    return latlon_idx\n",
        "\n",
        "def fetch_NBMgrib_from_AWS(iter_item, save_dir='nbm_grib2/', **req):\n",
        "\n",
        "    from botocore import UNSIGNED\n",
        "    from botocore.client import Config\n",
        "\n",
        "    nbm_sets = ['qmd'] #, 'core']\n",
        "\n",
        "    # As strings\n",
        "    yyyymmdd = iter_item[:-4]\n",
        "    hh = iter_item[-4:-2]\n",
        "\n",
        "    element_label = req['element'] if req['element'] != 'qpf' else \\\n",
        "                    req['element'] + f'{req[\"interval\"]:02d}'\n",
        "\n",
        "    save_dir = mkdir_p(save_dir)\n",
        "\n",
        "    output_file = (save_dir +\n",
        "        f'{yyyymmdd}.t{hh}z.fhr{req[\"lead_time_days\"]*24:03d}.{element_label}.grib2')\n",
        "\n",
        "    if os.path.isfile(output_file):\n",
        "        pass\n",
        "        # return output_file\n",
        "\n",
        "    else:\n",
        "        client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "        for nbm_set in nbm_sets:\n",
        "\n",
        "            bucket_dir = f'blend.{yyyymmdd}/{hh}/{nbm_set}/'\n",
        "\n",
        "            grib_file = f'{bucket_dir}blend.t{hh}z.'+\\\n",
        "                        f'{nbm_set}.f{req[\"lead_time_days\"]*24:03d}.{req[\"nbm_area\"]}.grib2'\n",
        "\n",
        "            index_file = f'{grib_file}.idx'\n",
        "\n",
        "            try:\n",
        "                index_data_raw = client.get_object(\n",
        "                    Bucket=aws_bucket_nbm, Key=index_file)['Body'].read().decode().split('\\n')\n",
        "\n",
        "            except:\n",
        "                client.close()\n",
        "                return\n",
        "\n",
        "            cols = ['num', 'byte', 'date', 'var', 'level',\n",
        "                'forecast', 'fthresh', 'ftype', '']\n",
        "\n",
        "            n_data_cols = len(index_data_raw[0].split(':'))\n",
        "\n",
        "        while len(cols) > n_data_cols:\n",
        "            cols = cols[:-1]\n",
        "\n",
        "        index_data = pd.DataFrame(\n",
        "            [item.split(':') for item in index_data_raw],\n",
        "                        columns=cols)\n",
        "\n",
        "        # Clean up any ghost indicies, set the indexA\n",
        "        index_data = index_data[index_data['num'] != '']\n",
        "        index_data['num'] = index_data['num'].astype(int)\n",
        "        index_data = index_data.set_index('num')\n",
        "\n",
        "        # Allow byte ranging to '' (EOF)\n",
        "        index_data.loc[index_data.shape[0]+1] = ['']*index_data.shape[1]\n",
        "\n",
        "        # Isolate the correct forecast interval\n",
        "        if req['element'] == 'qpf':\n",
        "            index_data = index_data.query('byte != \"\"')\n",
        "\n",
        "            forecast_step = []\n",
        "            for item in index_data['forecast']:\n",
        "                step, steptype = item.replace(' acc fcst', '').split(' ')\n",
        "                step = np.array(step.split('-')).astype(int)\n",
        "                step = f'{(step[-1] - step[0]):02d}'\n",
        "\n",
        "                if ((steptype == 'day') & (step == '01')):\n",
        "                    step, steptype = '24', 'hour'\n",
        "\n",
        "                forecast_step.append(step)\n",
        "\n",
        "            index_data.insert(4, 'step', forecast_step)\n",
        "\n",
        "        index_subset = index_data[\n",
        "            ((index_data['var'] == req['var']) &\n",
        "            (index_data['level'] == req['level']))]\n",
        "\n",
        "        if req['element'] == 'qpf':\n",
        "            index_subset = index_subset[\n",
        "                index_data['step'] == f\"{req['interval']:02d}\"]\n",
        "\n",
        "        # Depreciated, old pandas style\n",
        "        # byte start >> byte range\n",
        "        # for i in index_subset.index:\n",
        "        #     try:\n",
        "        #         index_data.loc[int(i)+1]\n",
        "        #     except:\n",
        "        #         index_subset.loc[i]['byte'] = [\n",
        "        #             index_data.loc[i, 'byte'], '']\n",
        "        #     else:\n",
        "        #         index_subset.loc[i]['byte'] = [\n",
        "        #             index_data.loc[i, 'byte'],\n",
        "        #             index_data.loc[i+1, 'byte']]\n",
        "\n",
        "        # byte start >> byte range\n",
        "        for i in index_subset.index:\n",
        "            try:\n",
        "                index_data.loc[int(i)+1]\n",
        "            except:\n",
        "                index_subset['byte'][i] = (\n",
        "                    index_data.loc[i, 'byte'], '')\n",
        "            else:\n",
        "                index_subset['byte'][i] = (\n",
        "                    index_data.loc[i, 'byte'],\n",
        "                    index_data.loc[i+1, 'byte'])\n",
        "\n",
        "        # Filter out excluded vars\n",
        "        for ex in excludes:\n",
        "            mask = np.column_stack([index_subset[col].str.contains(ex, na=False)\n",
        "                                    for col in index_subset])\n",
        "\n",
        "            index_subset = index_subset.loc[~mask.any(axis=1)]\n",
        "\n",
        "        # Fetch the data by byte range, write from stream\n",
        "        for index, item in index_subset.iterrows():\n",
        "            byte_range = f\"bytes={item['byte'][0]}-{item['byte'][1]}\"\n",
        "\n",
        "            output_bytes = client.get_object(\n",
        "                Bucket=aws_bucket_nbm, Key=grib_file, Range=byte_range)\n",
        "\n",
        "            with open(output_file, 'ab') as wfp:\n",
        "                for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                    wfp.write(chunk)\n",
        "\n",
        "        client.close()\n",
        "\n",
        "def fetch_URMAgrib_from_AWS(iter_item, save_dir='urma_grib2/', **req):\n",
        "    from botocore import UNSIGNED\n",
        "    from botocore.client import Config\n",
        "\n",
        "    save_dir = mkdir_p(save_dir)\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    if req[\"element\"] == 'maxt':\n",
        "        hh_set = [8]\n",
        "\n",
        "    elif req[\"element\"] == 'mint':\n",
        "        hh_set = [20]\n",
        "\n",
        "    elif req[\"element\"] == 'qpf':\n",
        "        if req[\"interval\"] == 6:\n",
        "            # Change this if aggregating all 4 runs/only one run\n",
        "            hh_set = [0, 12]\n",
        "\n",
        "        elif req[\"interval\"] == 12:\n",
        "            # Change this if aggregating all 4 runs/only one run\n",
        "            hh_set = [0, 6, 12, 18]\n",
        "\n",
        "        else:\n",
        "            hh_set = [0, 6, 12, 18]\n",
        "\n",
        "    client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "    element_label = element if element != 'qpf' else 'qpe06'\n",
        "\n",
        "    for hh in hh_set:\n",
        "\n",
        "        output_file = (save_dir +\n",
        "            f'urma2p5.{yyyymmdd}.t{hh:02d}z.{element_label}.grib2')\n",
        "\n",
        "        if os.path.isfile(output_file):\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Buffering the dates can be done outside the loop when building iterable\n",
        "            bucket_dir = f'urma2p5.{yyyymmdd}/'\n",
        "\n",
        "            if element == 'qpf':\n",
        "                grib_file = f'{bucket_dir}urma2p5.{yyyymmdd}{hh:02d}.pcp_06h.wexp.grb2'\n",
        "            else:\n",
        "                grib_file = f'{bucket_dir}urma2p5.t{hh:02d}z.2dvaranl_ndfd.grb2_wexp'\n",
        "\n",
        "            try:\n",
        "                output_bytes = client.get_object(Bucket=aws_bucket_urma, Key=grib_file)\n",
        "            except:\n",
        "                pass\n",
        "            else:\n",
        "                with open(output_file, 'ab') as wfp:\n",
        "                    for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                        wfp.write(chunk)\n",
        "\n",
        "    client.close()\n",
        "\n",
        "def extract_nbm_value(grib_index, nbm_data):\n",
        "    return nbm_data[grib_index]\n",
        "\n",
        "def brier_skill_score(_y_test, _y_prob):\n",
        "\n",
        "    _y_ref = _y_test.sum()/y_test.size\n",
        "\n",
        "    bss = 1 - (brier_score_loss(_y_test, _y_prob) /\n",
        "                brier_score_loss(_y_test, np.full(_y_test.shape, _y_ref)))\n",
        "\n",
        "    return bss\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# INPUT-BASED GLOBAL VARIABLES AND CONFIG                                     #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "valid_date_iterable = []\n",
        "valid_datetime_iterable = []\n",
        "forecast_datetime_iterable = []\n",
        "\n",
        "while iter_date <= end_date:\n",
        "\n",
        "    valid_date_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "\n",
        "    for hour_range in ob_hours[element]:\n",
        "        end_hour = hour_range[-1]\n",
        "\n",
        "        valid_datetime_iterable.append(iter_date.strftime('%Y%m%d') + end_hour)\n",
        "\n",
        "        forecast_datetime_iterable.append(\n",
        "                (iter_date-timedelta(days=lead_days_selection)\n",
        "            ).strftime('%Y%m%d') + end_hour)\n",
        "\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "if region_selection == 'CWA':\n",
        "    cwa_query = cwa_selection\n",
        "elif region_selection == 'RFC':\n",
        "    rfc_metadata = cwa_list_rfc(cwa_selection)\n",
        "    cwa_query = ','.join([str(cwa) for cwa in rfc_metadata['CWA'].unique()\n",
        "                if cwa is not None])\n",
        "else:\n",
        "    cwa_query = ','.join(cwa_list(region_selection))\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# DATA ACQUISITION                                                            #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "multiprocess_function = partial(fetch_obs_from_API,\n",
        "                                cwa=cwa_query,\n",
        "                                **synoptic_api_args)\n",
        "\n",
        "# Multithreaded requests currently not supported by the Synoptic API\n",
        "for iter_item in valid_datetime_iterable:\n",
        "    multiprocess_function(iter_item)\n",
        "\n",
        "# with Pool(process_pool_size) as pool:\n",
        "#     print(f'Spooling up process pool for {len(valid_datetime_iterable)} tasks '\n",
        "#           f'across {process_pool_size} workers')\n",
        "\n",
        "#     retry = pool.map(multiprocess_function, valid_datetime_iterable)\n",
        "#     pool.terminate()\n",
        "\n",
        "#     print('Multiprocessing Complete')\n",
        "\n",
        "# Glob together csv files\n",
        "# Need to filter by variable/region in case of region change or re-run!\n",
        "synoptic_varname = synoptic_vars_out[element]\n",
        "\n",
        "csv_element = element if element != 'qpf' else f'qpe{interval_selection:02d}'\n",
        "\n",
        "searchstring = (f'*{csv_element}*{region_selection}*.csv'\n",
        "    if region_selection != 'CWA' else f'*{csv_element}*{cwa_selection}*.csv')\n",
        "\n",
        "filelist = np.array(glob(os.path.join(output_dir + 'obs_csv/', searchstring)))\n",
        "\n",
        "datecheck = np.array(\n",
        "    [datetime.strptime(f.split('.')[-3], \"%Y%m%d%H%M\") for f in filelist])\n",
        "\n",
        "datecheck_mask = np.where(\n",
        "    (datecheck >= start_date.replace(hour=0, minute=0))\n",
        "    & (datecheck <= end_date.replace(hour=23, minute=59)))\n",
        "\n",
        "filelist = filelist[datecheck_mask]\n",
        "\n",
        "df = pd.concat(map(pd.read_csv, filelist),\n",
        "            ignore_index=True)\n",
        "\n",
        "if element == 'qpf':\n",
        "    # Un-nest precipitation observations\n",
        "    # df_qpf = pd.concat([pd.DataFrame(json.loads(row.replace(\"'\", '\"')))\n",
        "    #         for row in df[synoptic_varname]], ignore_index=True)\n",
        "\n",
        "    # df = df.drop(columns=synoptic_varname).join(df_qpf)\n",
        "\n",
        "    # # Rename the variable since we've changed the column name\n",
        "    print('Un-nesting precipitation observations')\n",
        "\n",
        "    # qpf_df = []\n",
        "    # for row in df.iterrows():\n",
        "    #     row = row[1]\n",
        "\n",
        "    #     _qpf_df = pd.DataFrame(eval(row[synoptic_varname]))\n",
        "\n",
        "    #     if 'CWA' in df.columns:\n",
        "    #         _qpf_df.insert(0, 'STATE', row['STATE'])\n",
        "    #         _qpf_df.insert(0, 'CWA', row['CWA'])\n",
        "\n",
        "    #     _qpf_df.insert(0, 'ELEVATION', row['ELEVATION'])\n",
        "    #     _qpf_df.insert(0, 'LONGITUDE', row['LONGITUDE'])\n",
        "    #     _qpf_df.insert(0, 'LATITUDE', row['LATITUDE'])\n",
        "    #     _qpf_df.insert(0, 'STID', row['STID'])\n",
        "\n",
        "    #     qpf_df.append(_qpf_df)\n",
        "\n",
        "    # Rename the variable since we've changed the column name\n",
        "    # synoptic_varname = 'total'\n",
        "\n",
        "    # print('Concatenating DataFrame')\n",
        "    # df = pd.concat(qpf_df).reset_index()\n",
        "    # df['last_report'] = pd.to_datetime(df['last_report']).round('6H')\n",
        "\n",
        "    # df['total'] = df[synoptic_varname].apply(lambda x:\n",
        "    #     json.loads(\n",
        "    #         x[1:-1].replace(\"'\",'\"'))['total']\n",
        "    # ).astype(float)\n",
        "\n",
        "    df['last_report'] = pd.to_datetime(df[synoptic_varname].apply(\n",
        "            lambda x: json.loads(x[1:-1].replace(\"'\",'\"'))['last_report'])\n",
        "        ).dt.round('6H')\n",
        "\n",
        "    synoptic_varname = 'TOTAL'\n",
        "\n",
        "# Identify the timestamp column (changes with variable)\n",
        "for k in df.keys():\n",
        "    if (('date_time' in k) or ('last_report' in k)):\n",
        "        time_col = k\n",
        "\n",
        "df.rename(columns={time_col:'timestamp'}, inplace=True)\n",
        "time_col = 'timestamp'\n",
        "\n",
        "# Convert read strings to datetime object\n",
        "df[time_col] = pd.to_datetime(df['timestamp']).round('60min')\n",
        "\n",
        "# DATETIME OFFSET FIX FOR TEMPS\n",
        "if element == 'maxt':\n",
        "    # Attribute to the day prior if UTC < 06Z otherwise attribute as stamped\n",
        "    df['timestamp'] = df['timestamp'].where(df['timestamp'].dt.hour < 8,\n",
        "                    df['timestamp'] - pd.Timedelta(1, unit='D'))\n",
        "\n",
        "    # Falls outside of URMA and NBM timeframe for MaxT\n",
        "    df['MAXT'] = df['timestamp'].where(\n",
        "        ((df['timestamp'].dt.hour >= 8)&(df['timestamp'].dt.hour < 12)), np.nan)\n",
        "\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp']) + timedelta(hours=8)\n",
        "\n",
        "elif element == 'mint':\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp']) + timedelta(hours=20)\n",
        "\n",
        "# Drop any NaNs and sort by date with station as secondary index\n",
        "df.set_index(['timestamp'], inplace=True)\n",
        "df = df[df.index.notnull()].reset_index().set_index(['timestamp', 'STID'])\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "if 'CWA' in df.columns:\n",
        "    df = df[['CWA', 'STATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', synoptic_varname]]\n",
        "else:\n",
        "    df = df[['LATITUDE', 'LONGITUDE', 'ELEVATION', synoptic_varname]]\n",
        "\n",
        "# df = df.rename(columns={synoptic_varname:element.upper()})\n",
        "df = df.rename(columns={synoptic_varname:'OBS'})\n",
        "\n",
        "if region_selection == 'RFC':\n",
        "    # Subset the indices to match\n",
        "    df_set = set(df.index.get_level_values(1).unique())\n",
        "    rfc_set = set(rfc_metadata.index.get_level_values(1).unique())\n",
        "    rfc_site_index = np.array(list(df_set.intersection(rfc_set)), dtype=str)\n",
        "    df = df[df.index.get_level_values(1).isin(rfc_site_index)]\n",
        "\n",
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= (end_date + timedelta(days=2)):\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_NBMgrib_from_AWS, **nbm_request_args)\n",
        "\n",
        "# Set up this way for later additions (e.g. a 2D iterable)\n",
        "# multiprocess_iterable = [item for item in itertools.product(\n",
        "#     other_iterable, date_selection_iterable)]\n",
        "\n",
        "multiprocess_iterable = forecast_datetime_iterable\n",
        "\n",
        "# for iter_item in multiprocess_iterable:\n",
        "#     multiprocess_function(iter_item)\n",
        "\n",
        "# with get_context('fork').Pool(process_pool_size) as pool:\n",
        "with Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} NBM tasks '\n",
        "        f'across {process_pool_size} workers')\n",
        "    NBMgrib_output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "    print('Multiprocessing Complete')\n",
        "\n",
        "# Gridded URMA pull for verification using NBM pull framework (AWS)\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_URMAgrib_from_AWS, **nbm_request_args)\n",
        "multiprocess_iterable = date_selection_iterable\n",
        "\n",
        "# with get_context('fork').Pool(process_pool_size) as pool:\n",
        "with Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} URMA tasks '\n",
        "        f'across {process_pool_size} workers')\n",
        "    URMAgrib_output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "\n",
        "print('Multiprocessing Complete')\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# EXTRACT DATA AND CALCULATE STATISTICS                                       #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Loop over dates in the DataFrame, open one NBM file at a time\n",
        "for valid_date in df.index.get_level_values(0).unique():\n",
        "\n",
        "    # We are looping over the VALID DATE... the filenames are stamped\n",
        "    # with the INIT DATE. We need to offset the valid dates to work!\n",
        "    init_date = valid_date - pd.Timedelta(\n",
        "        nbm_request_args['lead_time_days'], 'day')\n",
        "\n",
        "    if ((element == 'maxt') or (element == 'mint')):\n",
        "        init_hour = 6 if element == 'maxt' else 18\n",
        "\n",
        "    else:\n",
        "        init_hour = init_date.hour\n",
        "\n",
        "    datestr = datetime.strftime(init_date, '%Y%m%d')\n",
        "\n",
        "    # # # # # # # # # # #\n",
        "    # File Arrangement\n",
        "\n",
        "    element_name = f'{element}{interval_selection:02d}' if element == 'qpf' \\\n",
        "                        else f'{element}'\n",
        "\n",
        "    nbm_file = output_dir + f'nbm_grib2/{datestr}.t{init_hour:02d}z' +\\\n",
        "            f'.fhr{nbm_request_args[\"lead_time_days\"]*24:03d}' +\\\n",
        "            f'.{element_name}.grib2'\n",
        "\n",
        "    if element == 'qpf':\n",
        "        urma_name = 'QPE_URMA'\n",
        "\n",
        "        if interval_selection == 6:\n",
        "            valid_set = [valid_date]\n",
        "\n",
        "        if interval_selection == 12:\n",
        "            valid_set = [valid_date - timedelta(hours=offset)\n",
        "                for offset in [6, 0]]\n",
        "\n",
        "        elif interval_selection == 24:\n",
        "            valid_set = [valid_date - timedelta(hours=offset)\n",
        "                for offset in [18, 12, 6, 0]]\n",
        "\n",
        "    elif element == 'maxt':\n",
        "        urma_name = 'MAXT_URMA'\n",
        "        valid_set = [valid_date]\n",
        "\n",
        "    elif element == 'mint':\n",
        "        urma_name = 'MINT_URMA'\n",
        "        valid_set = [valid_date]\n",
        "\n",
        "    # # # # # # # # # # #\n",
        "    # Data Extraction (NBM)\n",
        "\n",
        "    # print(nbm_file)\n",
        "\n",
        "    if os.path.isfile(nbm_file):\n",
        "        nbm = pygrib.open(nbm_file)\n",
        "\n",
        "        print(f'\\nextracting i:{init_date}, nbf:{nbm_file}')\n",
        "\n",
        "        # If not yet indexed, go ahead and build the indexer\n",
        "        if 'grib_index' not in df.columns:\n",
        "\n",
        "            nbmlats, nbmlons = nbm.message(1).latlons()\n",
        "\n",
        "            df_indexed = df.reset_index()[\n",
        "                ['STID', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].drop_duplicates()\n",
        "\n",
        "            ll_to_index_mapped = partial(ll_to_index,\n",
        "                                        datalats=nbmlats, datalons=nbmlons)\n",
        "\n",
        "            print('\\nFirst pass: creating y/x grib indicies from lat/lon\\n')\n",
        "\n",
        "            df_indexed['grib_index'] = df_indexed.swifter.apply(\n",
        "                lambda x: ll_to_index_mapped(x.LATITUDE, x.LONGITUDE), axis=1)\n",
        "\n",
        "            # Extract the grid latlon\n",
        "            extract_nbm_lats_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=nbmlats)\n",
        "\n",
        "            extract_nbm_lons_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=nbmlons)\n",
        "\n",
        "            df_indexed['grib_lat'] = df_indexed['grib_index'].apply(\n",
        "                extract_nbm_lats_mapped)\n",
        "\n",
        "            df_indexed['grib_lon'] = df_indexed['grib_index'].apply(\n",
        "                extract_nbm_lons_mapped)\n",
        "\n",
        "            df_indexed.set_index('STID', inplace=True)\n",
        "\n",
        "            # Testing QC for non-unique indicies\n",
        "            df = df.reset_index().drop_duplicates(\n",
        "                    subset=['timestamp', 'STID'], keep='first'\n",
        "                ).set_index(['timestamp', 'STID'])\n",
        "\n",
        "            df = df.reset_index('timestamp').join(\n",
        "                    df_indexed[['grib_index', 'grib_lat', 'grib_lon']]\n",
        "                ).reset_index().set_index(\n",
        "                    ['timestamp', 'STID']).sort_index()\n",
        "\n",
        "        # Extract the data for that date and re-insert into DataFrame\n",
        "        # Loop over each variable in the NBM file and store to DataFrame\n",
        "        # May need a placeholder column of NaNs in df for each var to make this work...\n",
        "        # Use .swifter.apply() as needed if this will speed up the process\n",
        "        # Alternatively, can use multiprocess pool to thread out the work over each date\n",
        "        # First pass this seems fast enough as it is...\n",
        "        for msg in nbm:\n",
        "            if (('Probability' in str(msg)) & (('temperature' in str(msg)) or\n",
        "                ((msg.lengthOfTimeRange == interval_selection)))):\n",
        "\n",
        "                # Deal with column names\n",
        "                if (('Precipitation' in str(msg)) &\n",
        "                (msg.lengthOfTimeRange == interval_selection)):\n",
        "\n",
        "                    threshold_in = round(msg['upperLimit']*0.0393701, 2)\n",
        "\n",
        "                    name = f\"tp_ge_{str(threshold_in).replace('.','p')}\"\n",
        "\n",
        "                elif 'temperature' in str(msg):\n",
        "                    gtlt = 'le' if 'below' in str(msg) else 'ge'\n",
        "                    tk = (msg['lowerLimit'] if 'below'\n",
        "                            in str(msg) else msg['upperLimit'])\n",
        "                    tk = tk_fix[tk]\n",
        "                    tc = tk-273\n",
        "                    tf = (((tc)*(9/5))+32)\n",
        "                    name = f\"temp_{gtlt}_{tf:.0f}\".replace('-', 'm')\n",
        "\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                df.loc[valid_date, name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                    extract_nbm_value_mapped).values\n",
        "\n",
        "            elif 'temperature at 2 metres' in str(msg): # OR precipitation clause\n",
        "                name = 'FXMAXT' if element == 'maxt' else 'FXMINT'\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                # Convert to F from K\n",
        "                df.loc[valid_date, name] = (((df.loc[valid_date]['grib_index'].apply(\n",
        "                    extract_nbm_value_mapped).values - 273.15)*(9/5))+32)\n",
        "\n",
        "            elif (('Precipitation' in str(msg)) &\n",
        "                (msg.lengthOfTimeRange == interval_selection)):\n",
        "\n",
        "                name = 'FXQPF'\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                df.loc[valid_date, name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                                    extract_nbm_value_mapped).values\n",
        "\n",
        "        nbm.close()\n",
        "\n",
        "        # # # # # # # # # # #\n",
        "        # Data Extraction (URMA)\n",
        "\n",
        "        # lat shape lon shape are 2d and interchangable\n",
        "        msg = np.zeros(nbmlats.shape) if element == 'qpf' else None\n",
        "\n",
        "        urma_element = element if element != 'qpf' else 'qpe06'\n",
        "\n",
        "        baddata = False\n",
        "        for urma_datetime in valid_set:\n",
        "\n",
        "            urma_file = output_dir + f'urma_grib2/urma2p5.' +\\\n",
        "                f'{(urma_datetime).strftime(\"%Y%m%d\")}.'+\\\n",
        "                f't{urma_datetime.hour:02d}z.{urma_element}.grib2'\n",
        "\n",
        "            print(f'extracting v:{valid_date}, urf:{urma_file}')\n",
        "\n",
        "            if os.path.isfile(urma_file):\n",
        "                urma = pygrib.open(urma_file)\n",
        "\n",
        "                if element == 'qpf':\n",
        "                    # Sum onto the initalized zero array\n",
        "                    urma = pygrib.open(urma_file)\n",
        "                    try:\n",
        "                        msg += urma.select(shortName='tp')[0].values\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                elif element == 'maxt':\n",
        "                    msg = ((urma.select(shortName='tmax')[0].values - 273.15) * (9/5)) + 32\n",
        "\n",
        "                elif element == 'mint':\n",
        "                    msg = ((urma.select(shortName='tmin')[0].values - 273.15) * (9/5)) + 32\n",
        "\n",
        "                urma.close()\n",
        "\n",
        "            else:\n",
        "                print(f'{urma_file} not found, skipping and backfill nan\\n')\n",
        "                baddata = True\n",
        "\n",
        "        if urma_name not in df.columns:\n",
        "            df[urma_name] = np.nan\n",
        "\n",
        "        msg = msg if baddata == False else np.full(nbmlats.shape, np.nan)\n",
        "\n",
        "        extract_urma_value_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=(msg))\n",
        "\n",
        "        df.loc[valid_date, urma_name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                            extract_urma_value_mapped).values\n",
        "\n",
        "# QC Checks\n",
        "# if element == 'qpf':\n",
        "#     # Constrain potentially spurious obs... probably better ways but works\n",
        "#     df.loc[df['OBS'] > df['QPE_URMA'], 'OBS'] = np.nan\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# GENERATE FIGURES AND TABLES                                                 #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "C1, C2 = 'Lime', 'Magenta'\n",
        "\n",
        "df = df.dropna(how='any')\n",
        "df.index.get_level_values(0).unique()\n",
        "\n",
        "varname = element.upper()\n",
        "urma_varname = varname if element != 'qpf' else 'QPE'\n",
        "\n",
        "keylist = [k for k in df.columns if (('ge' in k) or ('le' in k))]\n",
        "\n",
        "# threshlist = [float(\n",
        "#     k.split('_')[-1].replace('p', '.').replace('m', '-')) for k in keylist]\n",
        "\n",
        "# if element == 'qpf':\n",
        "#     threshlist = [t*25.4 for t in threshlist]\n",
        "\n",
        "# for i, t in enumerate(zip(keylist, threshlist)):\n",
        "ziplist = []\n",
        "for k in sorted(keylist):\n",
        "\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(6, 8))\n",
        "    gs = fig.add_gridspec(nrows=4, ncols=3, left=0.05, right=0.5,\n",
        "                            hspace=-0.05, wspace=0.05)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[:-1, :])\n",
        "    ax2 = fig.add_subplot(gs[-1, :-1])\n",
        "    ax3 = fig.add_subplot(gs[-1, -1])\n",
        "\n",
        "    # thresh_text, thresh = t\n",
        "    thresh_text = k\n",
        "    thresh = float(k.split('_')[-1].replace('p', '.').replace('m', '-'))\n",
        "    thresh = thresh*25.4 if element == 'qpf' else thresh\n",
        "\n",
        "    print(thresh_text)\n",
        "\n",
        "    if 'ge' in thresh_text:\n",
        "        y_test_ob = np.where(df['OBS'] >=  thresh, 1, 0) #ge vs gt\n",
        "        y_test = np.where(df[f'{urma_varname}_URMA'] >= thresh, 1, 0)\n",
        "\n",
        "    elif 'le' in thresh_text:\n",
        "        y_test_ob = np.where(df['OBS'] <=  thresh, 1, 0) #le vs lt\n",
        "        y_test = np.where(df[f'{urma_varname}_URMA'] <= thresh, 1, 0)\n",
        "\n",
        "    y_prob = df[thresh_text]/100\n",
        "    y_pred = np.where(df['FX'+varname] >= thresh, 1, 0) #ge vs gt\n",
        "\n",
        "    x_ref = y_prob.sum()/y_prob.size\n",
        "    y_ref = y_test.sum()/y_test.size\n",
        "    y_ref_ob = y_test_ob.sum()/y_test_ob.size\n",
        "\n",
        "    noskill_x, noskill_y = np.array([(bin, (bin + y_ref)/2)\n",
        "                                    for bin in np.arange(0, 1.1, .1)]).T\n",
        "\n",
        "    _, noskill_y_ob = np.array([(bin, (bin + y_ref_ob)/2)\n",
        "                                for bin in np.arange(0, 1.1, .1)]).T\n",
        "\n",
        "    # Calibration Curves/Reliability Diagrams\n",
        "    CalibrationDisplay.from_predictions(y_test, y_prob, n_bins=10, ax=ax1, name='URMA',\n",
        "                                        ref_line=False, marker='o', markersize=10,\n",
        "                                        markerfacecolor='none', linewidth=2, color=C1,\n",
        "                                        zorder=10)\n",
        "\n",
        "    CalibrationDisplay.from_predictions(y_test_ob, y_prob, n_bins=10, ax=ax1, name='OBS',\n",
        "                                        ref_line=False, marker='^', markersize=10,\n",
        "                                        markerfacecolor='none', linewidth=2, color=C2,\n",
        "                                        zorder=9)\n",
        "\n",
        "    # ax1.fill_between(np.linspace(x_ref, 1, noskill_y[noskill_y.round(3) >= x_ref.round(3)].size),\n",
        "    #                  noskill_y[noskill_y.round(3) >= x_ref.round(3)], 1,\n",
        "    #                  zorder=-10, color='gray', alpha=0.4)\n",
        "\n",
        "    # ax1.fill_between(np.linspace(x_ref, 1, noskill_y[noskill_y.round(3) >= x_ref.round(3)].size),\n",
        "    #                 y_ref, noskill_y[noskill_y.round(3) >= x_ref.round(3)],\n",
        "    #                 zorder=-10, color='gray', alpha=0.25)\n",
        "\n",
        "    # ax1.fill_between(np.linspace(0, x_ref, noskill_y[noskill_y.round(3) <= x_ref.round(3)].size), 0,\n",
        "    #                  noskill_y[noskill_y.round(3) <= x_ref.round(3)],\n",
        "    #                  zorder=-10, color='gray', alpha=0.4)\n",
        "\n",
        "    # ax1.fill_between(np.linspace(0, x_ref, noskill_y[noskill_y.round(3) <= x_ref.round(3)].size),\n",
        "    #                 noskill_y[noskill_y.round(3) <= x_ref.round(3)], y_ref,\n",
        "    #                 zorder=-10, color='gray', alpha=0.25)\n",
        "\n",
        "    ax1.plot(noskill_x, noskill_y, linestyle='--', linewidth=1.5, color='black', alpha=0.5, zorder=8)\n",
        "    # ax1.plot(noskill_x, noskill_y_ob, linestyle='--', color=C2)\n",
        "\n",
        "    ax1.axhline(y_ref, linestyle='--', linewidth=1.5, color='black', alpha=0.5, zorder=6)\n",
        "    # ax1.axhline(y_ref_ob, linestyle='--', linewidth=1.5, color=C2, alpha=0.5, zorder=-1)\n",
        "\n",
        "    ax1.axvline(x_ref, linestyle='--', linewidth=1.5, color='black', alpha=0.5, zorder=6)\n",
        "    ax1.plot([0, 1], [0, 1], '-', linewidth=1, color='black', alpha=1, zorder=7)\n",
        "\n",
        "    ax1.set_xticks(np.arange(0, 1.1, 0.1))\n",
        "    ax1.set_yticks(np.arange(0, 1.1, 0.1))\n",
        "\n",
        "    ax1.set_xlim([0, 1])\n",
        "    ax1.set_ylim([0, 1])\n",
        "\n",
        "    ax1.set_xlabel('Forecast Probability')\n",
        "    ax1.set_ylabel('Observed Frequency')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Sharpness Diagram\n",
        "    ax2.hist(y_prob, bins=10, density=True, rwidth=0.8, log=True, color='black')\n",
        "    ax2.set_xlabel('Forecast Probability')\n",
        "    ax2.set_ylabel('Relative Frequency')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # ROC-AUC Curves\n",
        "    RocCurveDisplay.from_predictions(y_test, y_prob, ax=ax3, name='URMA', color=C1)\n",
        "    RocCurveDisplay.from_predictions(y_test_ob, y_prob, ax=ax3, name='OBS', color=C2)\n",
        "\n",
        "    ax3.set_xlabel('False Positive Rate')\n",
        "    ax3.set_ylabel('True Positive Rate')\n",
        "\n",
        "    ax3.yaxis.set_label_position(\"right\")\n",
        "    ax3.yaxis.tick_right()\n",
        "\n",
        "    ax3.grid(True)\n",
        "    ax3.get_legend().remove()\n",
        "\n",
        "    # Skill Scores\n",
        "    scores = defaultdict(list)\n",
        "    for name in ['OBS', 'URMA']:\n",
        "\n",
        "        _y_test = y_test if name == 'URMA' else y_test_ob\n",
        "\n",
        "        scores[\"Classifier\"].append(name)\n",
        "\n",
        "        for metric in [brier_score_loss, brier_skill_score, roc_auc_score]: #log_loss\n",
        "            try:\n",
        "                score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
        "                scores[score_name].append(metric(_y_test, y_prob))\n",
        "            except:\n",
        "                scores[score_name].append(np.nan)\n",
        "\n",
        "        for metric in [recall_score, precision_score]: #f1_score\n",
        "            score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
        "            scores[score_name].append(metric(_y_test, y_pred))\n",
        "\n",
        "    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
        "    score_df.round(decimals=3)\n",
        "\n",
        "    score_df.rename(columns={'Brier  loss':'Brier Score',\n",
        "                            'Brier skill ':'Brier Skill',\n",
        "                            'Log loss':'Log Loss',\n",
        "                            'Roc auc ':'ROC AUC',\n",
        "                            'Recall ':'Recall\\n(POD)',\n",
        "                            'Precision ':'Precision\\n(1-FAR)'}, inplace=True)\n",
        "\n",
        "    ax1.table(cellText=score_df.values.round(3),\n",
        "        colWidths=[0.25]*len(score_df.columns),\n",
        "        rowLabels=score_df.index,\n",
        "        colLabels=score_df.columns,\n",
        "        cellLoc='center', rowLoc='center',\n",
        "        loc='bottom', bbox=[0., -1.075, 1., 0.25])\n",
        "\n",
        "    # Title/Labels\n",
        "    n_urma = y_test.sum()\n",
        "    n_ob = y_test_ob.sum()\n",
        "    n_sites = df.index.get_level_values(1).unique().size\n",
        "\n",
        "\n",
        "    locname = (cwa_selection if\n",
        "        ((region_selection == 'CWA') or (region_selection == 'RFC'))\n",
        "        else region_selection)\n",
        "\n",
        "    all_dates = df.index.get_level_values(0).unique()\n",
        "    label_start = all_dates[0].strftime('%Y%m%d%H')\n",
        "    label_end = all_dates[-1].strftime('%Y%m%d%H')\n",
        "\n",
        "    suptitle = (f'{locname} n_stations ({network_selection}): {n_sites}\\n'+\\\n",
        "                f'valid: {label_start} - {label_end} F{lead_days_selection*24:-03d}')\n",
        "\n",
        "    reformat = {'tp':'Total Precipitation', 'MAXT':'Max Temp', 'tmin':'Min Temp',\n",
        "            'ge':'', 'le':'', 'p':'.', 'm':'-', '_':' ', 'temp':''}\n",
        "\n",
        "    thresh_text_raw = thresh_text\n",
        "    thresh_text = thresh_text.split('_')\n",
        "    for k in reformat.keys():\n",
        "        thresh_text[2] = thresh_text[2].replace(k, reformat[k])\n",
        "\n",
        "    thresh_text = ' '.join([\n",
        "        thresh_text[0].replace(thresh_text[0], reformat[thresh_text[0]]),\n",
        "        thresh_text[1].replace(thresh_text[1], reformat[thresh_text[1]]),\n",
        "        thresh_text[2]])\n",
        "\n",
        "    if element == 'qpf':\n",
        "        ax1.set_title(suptitle + '\\n'*2 +\\\n",
        "                f'{varname}{interval_selection:02d} {thresh_text}\"' +\\\n",
        "                    f'\\nn_events (URMA/OBS): {n_urma}/{n_ob}')\n",
        "\n",
        "    elif ((element == 'maxt') or (element == 'mint')):\n",
        "        # Add elif to only assign F to temp if adding future vars\n",
        "        ax1.set_title(suptitle + '\\n'*2 +\\\n",
        "                f'{varname} {thresh_text}F\\nn_events (URMA/OBS): {n_urma}/{n_ob}')\n",
        "\n",
        "    if display_inline:\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        if varname == 'QPF':\n",
        "            plot_fname = (f'{locname}_{network_selection.replace(\"+\",\"-\")}'\n",
        "                        f'_{label_start}-{label_end}_{varname}{interval_selection:02d}'\n",
        "                        f'_{thresh_text_raw}.reliability.png')\n",
        "        else:\n",
        "            plot_fname = (f'{locname}_{network_selection.replace(\"+\",\"-\")}'\n",
        "                    f'_{label_start}-{label_end}_{varname}'\n",
        "                    f'_{thresh_text_raw}.reliability.png')\n",
        "\n",
        "        print(f'Plotting/Saving {plot_fname}')\n",
        "        plt.savefig(f'{plot_fname}', bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "\n",
        "        ziplist.append(plot_fname)\n",
        "\n",
        "if display_inline:\n",
        "    pass\n",
        "else:\n",
        "    zip_name = '_'.join(plot_fname.split('_')[:-3])+'.zip'\n",
        "    zip_files(ziplist, zip_name)\n",
        "\n",
        "    print('Starting download... Manually download zipfile from left sidebar if unsuccessful')\n",
        "    files.download(zip_name)"
      ],
      "metadata": {
        "id": "WDT0j5h_6r93",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}