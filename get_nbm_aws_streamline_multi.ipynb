{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy82qjxbW2s9ZaITgbv6aY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/get_nbm_aws_streamline_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mCU01BOdhCx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "!pip install pygrib\n",
        "\n",
        "import os, gc\n",
        "import boto3\n",
        "import pygrib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "from functools import partial\n",
        "from datetime import datetime, timedelta\n",
        "from multiprocessing import cpu_count, get_context\n",
        "\n",
        "from multiprocessing import set_start_method"
      ],
      "metadata": {
        "id": "ZP6Vq6urQJst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "RgfkSfCGhEe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiprocess settings\n",
        "process_pool_size = cpu_count()*8\n",
        "print(f'Process Pool Size: {process_pool_size}')\n",
        "\n",
        "# Define Globals\n",
        "aws_bucket = 'noaa-nbm-grib2-pds'\n",
        "\n",
        "# Where to place the grib file (subdirs can be added in local) (not used)\n",
        "# output_dir = './'\n",
        "\n",
        "# Which grib variables do each element correlate with\n",
        "element_var = {'qpf':'APCP',\n",
        "                  'maxt':'TMP',\n",
        "                  'mint':'TMP'}\n",
        "\n",
        "# Which grib levels do each element correlate with\n",
        "element_lev = {'qpf':'surface',\n",
        "               'maxt':'2 m above ground',\n",
        "               'mint':'2 m above ground'}\n",
        "\n",
        "# If a grib message contains any of these, exclude\n",
        "excludes = ['ens std dev', '% lev']\n",
        "\n",
        "# Fix MDL's bad kelvin thresholds...\n",
        "tk_fix = {233.0:233.15, 244.0:244.261, 249.0:249.817,\n",
        "    255.0:255.372, 270.0:270.928, 273.0:273.15, 299.0:299.817,\n",
        "    305.0:305.372, 310.0:310.928, 316.0:316.483, 322.0:322.039}"
      ],
      "metadata": {
        "id": "mlF-5hehQNbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "T7TMDADMhK5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir_p(path):\n",
        "    from pathlib import Path\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    return path"
      ],
      "metadata": {
        "id": "zro5VKBy0K6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_unzip(url, save_dir='./shapefiles/', chunk_size=128):\n",
        "    import requests\n",
        "    import zipfile\n",
        "\n",
        "    save_file = url.split('/')[-1]\n",
        "    save_path = mkdir_p(save_dir) + save_file\n",
        "\n",
        "    if not os.path.isfile(save_path):\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(save_path, 'wb') as fd:\n",
        "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "                fd.write(chunk)\n",
        "\n",
        "    unzipped_dir = mkdir_p(save_dir + save_file.replace('.zip', ''))\n",
        "\n",
        "    if len(os.listdir(unzipped_dir)) == 0:\n",
        "        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(unzipped_dir)\n",
        "\n",
        "    return (unzipped_dir)"
      ],
      "metadata": {
        "id": "wKWFZrCEBs_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro8LqKJLQB4r"
      },
      "outputs": [],
      "source": [
        "def fetch_grib_from_AWS(iter_item, save_dir='./grib2/', **req):\n",
        "    from botocore import UNSIGNED\n",
        "    from botocore.client import Config\n",
        "\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    nbm_sets = ['qmd'] #, 'core']\n",
        "\n",
        "    mkdir_p(save_dir)\n",
        "\n",
        "    output_file = (save_dir +\n",
        "        f'{yyyymmdd}.t{req[\"hh\"]:02d}z.fhr{req[\"fhr\"]:03d}.{req[\"var\"]}.grib2')\n",
        "\n",
        "    if os.path.isfile(output_file):\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        for nbm_set in nbm_sets:\n",
        "\n",
        "            bucket_dir = f'blend.{yyyymmdd}/{req[\"hh\"]:02d}/{nbm_set}/'\n",
        "\n",
        "            grib_file = f'{bucket_dir}blend.t{req[\"hh\"]:02d}z.'+\\\n",
        "                        f'{nbm_set}.f{req[\"fhr\"]:03d}.{req[\"nbm_area\"]}.grib2'\n",
        "\n",
        "            index_file = f'{grib_file}.idx'\n",
        "\n",
        "            client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "            index_data_raw = client.get_object(\n",
        "                Bucket=aws_bucket, Key=index_file)['Body'].read().decode().split('\\n')\n",
        "\n",
        "            cols = ['num', 'byte', 'date', 'var', 'level',\n",
        "                'forecast', 'fthresh', 'ftype', '']\n",
        "\n",
        "            n_data_cols = len(index_data_raw[0].split(':'))\n",
        "\n",
        "            while len(cols) > n_data_cols:\n",
        "                cols = cols[:-1]\n",
        "\n",
        "            index_data = pd.DataFrame(\n",
        "                [item.split(':') for item in index_data_raw],\n",
        "                            columns=cols)\n",
        "\n",
        "            # Clean up any ghost indicies, set the index\n",
        "            index_data = index_data[index_data['num'] != '']\n",
        "            index_data['num'] = index_data['num'].astype(int)\n",
        "            index_data = index_data.set_index('num')\n",
        "\n",
        "            # Allow byte ranging to '' (EOF)\n",
        "            index_data.loc[index_data.shape[0]+1] = ['']*index_data.shape[1]\n",
        "\n",
        "            index_subset = index_data[\n",
        "                ((index_data['var'] == req['var']) &\n",
        "                (index_data['level'] == req['level']))]\n",
        "\n",
        "            # byte start >> byte range\n",
        "            for i in index_subset.index:\n",
        "                index_subset.loc[i]['byte'] = (\n",
        "                    index_data.loc[i, 'byte'],\n",
        "                    index_data.loc[int(i)+1, 'byte'])\n",
        "\n",
        "            # Filter out excluded vars\n",
        "            for ex in excludes:\n",
        "                mask = np.column_stack([index_subset[col].str.contains(ex, na=False)\n",
        "                                        for col in index_subset])\n",
        "\n",
        "                index_subset = index_subset.loc[~mask.any(axis=1)]\n",
        "\n",
        "            # Fetch the data by byte range, write from stream\n",
        "            for index, item in index_subset.iterrows():\n",
        "                byte_range = f\"bytes={item['byte'][0]}-{item['byte'][1]}\"\n",
        "\n",
        "                output_bytes = client.get_object(\n",
        "                    Bucket=aws_bucket, Key=grib_file, Range=byte_range)\n",
        "\n",
        "                with open(output_file, 'ab') as wfp:\n",
        "                    for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                        wfp.write(chunk)\n",
        "\n",
        "    return output_file\n",
        "    client.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_region_bounds(nws_region):\n",
        "    import geopandas as gpd\n",
        "\n",
        "    cwa_shapefile = download_unzip(\n",
        "        'https://www.weather.gov/source/gis/Shapefiles/WSOM/w_08mr23.zip')\n",
        "\n",
        "    cwas = gpd.read_file(cwa_shapefile)\n",
        "\n",
        "    nws_regions = ['WR', 'CR', 'ER', 'SR']\n",
        "\n",
        "    if nws_region in nws_regions:\n",
        "        bounds = cwas.query(f\"REGION == '{nws_region}'\").total_bounds\n",
        "    else:\n",
        "        bounds = cwas.query(f\"CWA == '{nws_region}'\").total_bounds\n",
        "\n",
        "    return bounds"
      ],
      "metadata": {
        "id": "ce078oeEHtuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grib2nc(grib_file_path, subset_bounds=None,\n",
        "                interval=False, save_dir='./netcdf/'):\n",
        "\n",
        "    mkdir_p(save_dir)\n",
        "\n",
        "    netcdf_file_path = (save_dir +\n",
        "    grib_file_path.split('/')[-1].replace('.grib2', '.nc'))\n",
        "\n",
        "    if not os.path.isfile(netcdf_file_path):\n",
        "\n",
        "        # Create a list to store the data arrays for each variable\n",
        "        data_arrays = []\n",
        "\n",
        "        # Open the GRIB2 file using pygrib\n",
        "        with pygrib.open(grib_file_path) as grib_file:\n",
        "            print('\\nreading: ', grib_file_path)\n",
        "\n",
        "            # Iterate over each message in the GRIB2 file\n",
        "            for msg in grib_file:\n",
        "\n",
        "                smsg = str(msg).lower()\n",
        "\n",
        "                grib_interval = msg['endStep'] - msg['startStep']\n",
        "\n",
        "                if (('probability' in smsg) &\n",
        "                    ((grib_interval == interval) or (not interval))):\n",
        "\n",
        "                    threshold_in = (round(msg['upperLimit']*0.0393701, 2)\n",
        "                                    if interval else 0)\n",
        "\n",
        "                    if (('temperature' in smsg) or (threshold_in <= 4.0)):\n",
        "\n",
        "                        valid_time = datetime.strptime(\n",
        "                            f\"{msg['validityDate']}{msg['validityTime']}\",\n",
        "                            '%Y%m%d%H%M')\n",
        "\n",
        "                        nlon, nlat, xlon, xlat = subset_bounds\n",
        "\n",
        "                        # Extract data and metadata from the GRIB2 message\n",
        "                        data = msg.values\n",
        "                        lats, lons = msg.latlons()\n",
        "\n",
        "                        # Less memory intensive method to subset on read but\n",
        "                        # returns 2 1D arrays (LCC projection??) and need 2D\n",
        "                        # data, lats, lons = msg.data(lat1=nlat, lat2=xlat,\n",
        "                        #                             lon1=nlon, lon2=xlon)\n",
        "\n",
        "                        # Create an xarray DataArray for the variable\n",
        "                        da = xr.DataArray(data,\n",
        "                                        coords={'lat': lats[:, 0],\n",
        "                                                'lon': lons[0, :]},\n",
        "                                        dims=['lat', 'lon'])\n",
        "\n",
        "                        da = da.sel(lat=slice(nlat, xlat),\n",
        "                                    lon=slice(nlon, xlon))\n",
        "                        gc.collect()\n",
        "\n",
        "                        # Add variable metadata as attributes (slow, not needed)\n",
        "                        # for key in msg.keys():\n",
        "                        #     if key not in ['values', 'latlons']:\n",
        "                        #         try:\n",
        "                        #             da.attrs[key] = msg[key]\n",
        "                        #         except:\n",
        "                        #             pass\n",
        "\n",
        "                        if 'precipitation' in smsg:\n",
        "                            da.name = f\"tp_ge_{str(threshold_in).replace('.','p')}\"\n",
        "\n",
        "                        elif 'temperature' in smsg:\n",
        "                            gtlt = 'le' if 'below' in smsg else 'ge'\n",
        "                            tk = (msg['lowerLimit'] if 'below'\n",
        "                                  in smsg else msg['upperLimit'])\n",
        "                            tk = tk_fix[tk]\n",
        "                            tc = tk-273\n",
        "                            tf = (((tc)*(9/5))+32)\n",
        "                            da.name = f\"temp_{gtlt}_{tf:.0f}\".replace('-', 'm')\n",
        "\n",
        "                        da['valid_time'] = valid_time\n",
        "\n",
        "                        # Add the DataArray to the list\n",
        "                        data_arrays.append(da)\n",
        "\n",
        "        # Combine the list of DataArrays into a single xarray dataset\n",
        "        ds = xr.merge(data_arrays, compat='override')\n",
        "        gc.collect()\n",
        "\n",
        "        ds.to_netcdf(netcdf_file_path)\n",
        "\n",
        "    return netcdf_file_path"
      ],
      "metadata": {
        "id": "aMbqBuI8ETMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Input/Multiprocessing Inputs"
      ],
      "metadata": {
        "id": "AuVwvcXWhMPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_selection = 'WR'\n",
        "element = 'qpf' #input('Desired element? (QPF/MaxT/MinT)').lower()\n",
        "\n",
        "start_date = '20220601'\n",
        "end_date = '20220610'\n",
        "\n",
        "# QPF 0/6/12/18, valid 0/6/12/18\n",
        "# MaxT 6/18 valid 6\n",
        "# MinT 6/18 valid 18\n",
        "\n",
        "# Build arg dict\n",
        "# !!! need to constrain Max min to 06/18 !!!\n",
        "nbm_request_args = {\n",
        "    #'yyyymmdd':yyyymmdd, #input('Desired init date (YYYYMMDD)? '),\n",
        "    'interval':24, #6/12/24/48/72, if element==temp then False\n",
        "    'hh':18, #int(input('Desired init hour int(HH)? ')),\n",
        "    'fhr':24, #int(input('Desired forecast hour/lead time int(HHH)?')),\n",
        "    'nbm_area':'co',\n",
        "    'var':element_var[element],\n",
        "    'level':element_lev[element]}"
      ],
      "metadata": {
        "id": "vtgrpivgQTNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main/Multiprocessing Call"
      ],
      "metadata": {
        "id": "Irb9HzFAhTZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+'0000', '%Y%m%d%H%M')\n",
        "    for date in [start_date, end_date]]\n",
        "\n",
        "# Fix offset of init time vs valid time to verify between chosen dates\n",
        "valid_hours_advance = nbm_request_args['hh'] + nbm_request_args['fhr']\n",
        "if (valid_hours_advance) >= 24:\n",
        "    start_date -= timedelta(days=int(valid_hours_advance/24))\n",
        "    end_date -= timedelta(days=int(valid_hours_advance/24))\n",
        "\n",
        "print(start_date, end_date)"
      ],
      "metadata": {
        "id": "ssFebRjVYfr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_grib_from_AWS, **nbm_request_args)\n",
        "\n",
        "# Set up this way for later additions (e.g. a 2D iterable)\n",
        "# multiprocess_iterable = [item for item in itertools.product(\n",
        "#     other_iterable, date_selection_iterable)]\n",
        "multiprocess_iterable = date_selection_iterable\n",
        "\n",
        "with get_context('fork').Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "          f'across {process_pool_size} workers')\n",
        "    grib_output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "    print('Multiprocessing Complete')"
      ],
      "metadata": {
        "id": "1-ReDYLChqO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_grib2nc = partial(grib2nc,\n",
        "                         subset_bounds=get_region_bounds(region_selection),\n",
        "                         interval=nbm_request_args['interval'])\n",
        "\n",
        "# netcdf_output_files = []\n",
        "# for grib_output_file in grib_output_files:\n",
        "#     netcdf_output_files.append(mp_grib2nc(grib_output_file))\n",
        "\n",
        "# Seems to behave OK with 4 procs, unstable higher than ?\n",
        "with get_context('fork').Pool(4) as pool:\n",
        "    netcdf_output_files = pool.map(mp_grib2nc, grib_output_files,\n",
        "                                   chunksize=1)\n",
        "    pool.terminate()\n",
        "\n",
        "# Compile along time axis\n",
        "nbm = xr.open_mfdataset(netcdf_output_files, combine='nested', concat_dim='time')\n",
        "nbm"
      ],
      "metadata": {
        "id": "dzlEEP0w9-iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract NBM Data at Observation Points into Pandas DataFrame"
      ],
      "metadata": {
        "id": "qOg1rMsxtXgA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2A6ljIH4hYMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}