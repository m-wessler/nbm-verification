{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVPJUEiKUN6nlaUNiwVKK7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/get_obs_synopticAPI_streamline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "RkIJ8-02HmyO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNqT9OOqHU7L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "from datetime import datetime, timedelta\n",
        "from multiprocessing import Pool, cpu_count"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "jj-yU-s5Hp5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiprocess settings\n",
        "process_pool_size = cpu_count()*30\n",
        "\n",
        "# Synoptic API token\n",
        "user_token = 'a2386b75ecbc4c2784db1270695dde73'\n",
        "\n",
        "# Backend APIs\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "qc_api = \"https://api.synopticdata.com/v2/stations/qcsegments?\"\n",
        "\n",
        "# Data Query APIs\n",
        "timeseries_api = \"https://api.synopticdata.com/v2/stations/timeseries?\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "\n",
        "# Assign API to element name\n",
        "synoptic_apis = {\n",
        "    'qpf':precipitation_api,\n",
        "    'maxt':statistics_api,\n",
        "    'mint':statistics_api}\n",
        "\n",
        "synoptic_networks = {\"NWS+RAWS+HADS\":\"1,2,106\",\n",
        "                     \"NWS+RAWS\":\"1,2\",\n",
        "                     \"NWS\":\"1\",\n",
        "                     \"RAWS\": \"2\",\n",
        "                     \"ALL\":\"\"}\n",
        "                    #  \"CUSTOM\": \"&network=\"+network_input,\n",
        "                    #  \"LIST\": \"&stid=\"+network_input}\n",
        "\n",
        "# Assign synoptic variable to element name\n",
        "synoptic_vars = {\n",
        "    'qpf':None,\n",
        "    'maxt':'air_temp',\n",
        "    'mint':'air_temp'}\n",
        "\n",
        "# Assign stat type to element name\n",
        "stat_type = {\n",
        "    'qpf':'total',\n",
        "    'maxt':'maximum',\n",
        "    'mint':'minimum'}\n",
        "\n",
        "ob_hours = {\n",
        "    'qpf':['1200', '1200'],\n",
        "    'maxt':['1200', '0600'],\n",
        "    'mint':['0000', '1800']}"
      ],
      "metadata": {
        "id": "zgsWPuZAHkKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "rZZJI1E8HqPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir_p(path):\n",
        "    from pathlib import Path\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    return path"
      ],
      "metadata": {
        "id": "9bEjGq_m2yi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cwa_list(input_region):\n",
        "\n",
        "    input_region = input_region.upper()\n",
        "\n",
        "    region_dict ={\n",
        "        \"WR\":[\"BYZ\", \"BOI\", \"LKN\", \"EKA\", \"FGZ\", \"GGW\", \"TFX\", \"VEF\", \"LOX\", \"MFR\",\n",
        "            \"MSO\", \"PDT\", \"PSR\", \"PIH\", \"PQR\", \"REV\", \"STO\", \"SLC\", \"SGX\", \"MTR\",\n",
        "            \"HNX\", \"SEW\", \"OTX\", \"TWC\"],\n",
        "\n",
        "        \"CR\":[\"ABR\", \"BIS\", \"CYS\", \"LOT\", \"DVN\", \"BOU\", \"DMX\", \"DTX\", \"DDC\", \"DLH\",\n",
        "            \"FGF\", \"GLD\", \"GJT\", \"GRR\", \"GRB\", \"GID\", \"IND\", \"JKL\", \"EAX\", \"ARX\",\n",
        "            \"ILX\", \"LMK\", \"MQT\", \"MKX\", \"MPX\", \"LBF\", \"APX\", \"IWX\", \"OAX\", \"PAH\",\n",
        "            \"PUB\", \"UNR\", \"RIW\", \"FSD\", \"SGF\", \"LSX\", \"TOP\", \"ICT\"],\n",
        "\n",
        "        \"ER\":[\"ALY\", \"LWX\", \"BGM\", \"BOX\", \"BUF\", \"BTV\", \"CAR\", \"CTP\", \"RLX\", \"CHS\",\n",
        "            \"ILN\", \"CLE\", \"CAE\", \"GSP\", \"MHX\", \"OKX\", \"PHI\", \"PBZ\", \"GYX\", \"RAH\",\n",
        "            \"RNK\", \"AKQ\", \"ILM\"],\n",
        "\n",
        "        \"SR\":[\"ABQ\", \"AMA\", \"FFC\", \"EWX\", \"BMX\", \"BRO\", \"CRP\", \"EPZ\", \"FWD\", \"HGX\",\n",
        "            \"HUN\", \"JAN\", \"JAX\", \"KEY\", \"MRX\", \"LCH\", \"LZK\", \"LUB\", \"MLB\", \"MEG\",\n",
        "            \"MAF\", \"MFL\", \"MOB\", \"MRX\", \"OHX\", \"LIX\", \"OUN\", \"SJT\", \"SHV\", \"TAE\",\n",
        "            \"TBW\", \"TSA\"]}\n",
        "\n",
        "    if input_region == \"CONUS\":\n",
        "        return np.hstack([region_dict[region] for region in region_dict.keys()])\n",
        "    else:\n",
        "        return region_dict[input_region]"
      ],
      "metadata": {
        "id": "fZ90FdUBHkNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_obs_from_API(date, cwa='', output_type='csv', use_saved=True, **req):\n",
        "\n",
        "    valid = True\n",
        "    cwa_filename = req['region'] if req['region'] else cwa\n",
        "\n",
        "    output_dir = mkdir_p(f'./obs_{output_type}/')\n",
        "\n",
        "    output_file = output_dir + f'obs.{req[\"element\"]}.{req[\"ob_stat\"]}' +\\\n",
        "                    f'.{date}.{cwa_filename}.{output_type}'\n",
        "\n",
        "    if os.path.isfile(output_file) & use_saved:\n",
        "        # print(f'Output file exists for:{iter_item}')\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        json_dir = mkdir_p('./obs_json/')\n",
        "\n",
        "        json_file = json_dir + f'obs.{req[\"element\"]}.{req[\"ob_stat\"]}' +\\\n",
        "                        f'.{date}.{cwa_filename}.json'\n",
        "\n",
        "\n",
        "        adjusted_end_date = (datetime.strptime(date, '%Y%m%d') +\n",
        "                            timedelta(days=req['days_offset'])\n",
        "                            ).strftime('%Y%m%d')\n",
        "\n",
        "        if os.path.isfile(json_file) & use_saved:\n",
        "            # print(f'Polling archived JSON for: {iter_item}')\n",
        "\n",
        "            with open(json_file, 'rb+') as rfp:\n",
        "                response_dataframe = pd.json_normalize(json.load(rfp)['STATION'])\n",
        "\n",
        "        else:\n",
        "            api_query_args = {\n",
        "                'api_token':f'&token={user_token}',\n",
        "                'station_query':f'&cwa={cwa}',\n",
        "                'network_query':f'&network={req[\"network_query\"]}',\n",
        "                'start_date_query':f'&start={date}{req[\"obs_start_hour\"]}',\n",
        "                'end_date_query':f'&end={adjusted_end_date}{req[\"obs_end_hour\"]}',\n",
        "                'vars_query':(f'&pmode=totals' if req[\"element\"] == 'qpf'\n",
        "                    else f'&vars={req[\"vars_query\"]}'),\n",
        "                'stats_query':f'&type={req[\"ob_stat\"]}',\n",
        "                'timezone_query':'&obtimezone=utc',\n",
        "                'api_extras':'&fields=name,status,latitude,longitude,elevation'}\n",
        "\n",
        "            api_query = req['api'] + ''.join(\n",
        "                [api_query_args[k] for k in api_query_args.keys()])\n",
        "\n",
        "            print(f'Polling API for: {iter_item}\\n{api_query}')\n",
        "\n",
        "            status_code, response_count = None, 0\n",
        "            while (status_code != 200) & (response_count <= 10):\n",
        "                print(f'{iter_item}, HTTP:{status_code}, #:{response_count}')\n",
        "\n",
        "                # Don't sleep first try, sleep increasing amount for each retry\n",
        "                time.sleep(2*response_count)\n",
        "\n",
        "                response = requests.get(api_query)\n",
        "                # response.raise_for_status()\n",
        "\n",
        "                status_code = response.status_code\n",
        "                response_count += 1\n",
        "\n",
        "            try:\n",
        "                response_dataframe = pd.json_normalize(\n",
        "                    response.json()['STATION'])\n",
        "            except:\n",
        "                valid = False\n",
        "            else:\n",
        "                with open(json_file, 'wb+') as wfp:\n",
        "                    wfp.write(response.content)\n",
        "\n",
        "        if valid:\n",
        "            # Check ACTIVE flag (Can disable in config above if desired)\n",
        "            response_dataframe = response_dataframe[\n",
        "                response_dataframe['STATUS'] == \"ACTIVE\"]\n",
        "\n",
        "            # Un-nest the QPF totals\n",
        "            if req['element'] == 'qpf':\n",
        "                response_dataframe['TOTAL'] = [i[0]['total']\n",
        "                    for i in response_dataframe['OBSERVATIONS.precipitation']]\n",
        "\n",
        "            if output_type == 'pickle':\n",
        "            # Save out df as pickle\n",
        "                response_dataframe.to_pickle(output_file)\n",
        "\n",
        "            elif output_type == 'csv':\n",
        "            # Save out df as csv\n",
        "                response_dataframe.to_csv(output_file)\n",
        "\n",
        "            return None\n",
        "\n",
        "        else:\n",
        "            return iter_item"
      ],
      "metadata": {
        "id": "pwOuWrEPR6sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Input/Multiprocessing Inputs"
      ],
      "metadata": {
        "id": "apDCRIwdHuu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect user inputs\n",
        "element = 'maxt'\n",
        "element = element.lower() # Failsafe\n",
        "\n",
        "region_selection = 'WR'\n",
        "cwa_selection = 'SLC'\n",
        "\n",
        "start_date = '20231101'\n",
        "end_date = '20231115'\n",
        "\n",
        "# Immediately convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+'0000', '%Y%m%d%H%M')\n",
        "    for date in [start_date, end_date]]"
      ],
      "metadata": {
        "id": "bUkwOkUtHkSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main/Multiprocessing Call"
      ],
      "metadata": {
        "id": "CNwevGb7Hy2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build arg dict\n",
        "synoptic_api_args = {\n",
        "    'obs_start_hour':ob_hours[element][0],\n",
        "    'obs_end_hour':ob_hours[element][1],\n",
        "    'ob_stat':stat_type[element],\n",
        "    'api':synoptic_apis[element],\n",
        "    'element':element,\n",
        "    'region':region_selection,\n",
        "    'network_query':synoptic_networks['NWS+RAWS'], # add config feature later\n",
        "    'vars_query':None if element == 'qpf'\n",
        "        else f'{synoptic_vars[element]}',\n",
        "    'days_offset':1 if element != 'mint' else 0}\n",
        "\n",
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "cwa_query = ','.join(cwa_list(region_selection)\n",
        "                    ) if region_selection is not None else cwa_selection\n",
        "\n",
        "multiprocess_function = partial(fetch_obs_from_API,\n",
        "                                cwa=cwa_query,\n",
        "                                **synoptic_api_args)"
      ],
      "metadata": {
        "id": "gtd6r2bWHz-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do the job\n",
        "for iter_item in date_selection_iterable:\n",
        "    multiprocess_function(iter_item)\n",
        "\n",
        "# with Pool(process_pool_size) as pool:\n",
        "#     print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "#           f'across {process_pool_size} workers')\n",
        "\n",
        "#     retry = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "#     pool.terminate()\n",
        "\n",
        "#     print('Multiprocessing Complete')"
      ],
      "metadata": {
        "id": "92NLkekkhfOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Glob together csv files\n",
        "# Need to filter by variable/region in case of region change or re-run!\n",
        "searchstring = (f'*{element}*{region_selection}*.csv'\n",
        "    if region_selection is not None else f'*{element}*{cwa_selection}*.csv')\n",
        "\n",
        "df = pd.concat(map(pd.read_csv, glob(os.path.join('./obs_csv/', searchstring))),\n",
        "               ignore_index=True)\n",
        "\n",
        "if element == 'qpf':\n",
        "    # Un-nest precipitation observations\n",
        "    df_qpf = pd.concat([pd.DataFrame(json.loads(row.replace(\"'\", '\"')))\n",
        "            for row in df['OBSERVATIONS.precipitation']], ignore_index=True)\n",
        "\n",
        "    df = df.drop(columns='OBSERVATIONS.precipitation').join(df_qpf)\n",
        "\n",
        "# Identify the timestamp column (changes with variable)\n",
        "for k in df.keys():\n",
        "    if (('date_time' in k) or ('last_report' in k)):\n",
        "        time_col = k\n",
        "\n",
        "df.rename(columns={time_col:'timestamp'}, inplace=True)\n",
        "time_col = 'timestamp'\n",
        "\n",
        "# Convert read strings to datetime object\n",
        "df[time_col] = pd.to_datetime(df['timestamp']).round('60min')\n",
        "\n",
        "if element == 'maxt':\n",
        "    # Attribute to the day prior if UTC < 06Z otherwise attribute as stamped\n",
        "    df['timestamp'] = df['timestamp'].where(df['timestamp'].dt.hour <= 6,\n",
        "                    df['timestamp']-pd.Timedelta(1, unit='D')).dt.date\n",
        "\n",
        "elif element == 'mint':\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "\n",
        "# Drop any NaNs and sort by date with station as secondary index\n",
        "df.set_index(['timestamp'], inplace=True)\n",
        "df = df[df.index.notnull()].reset_index().set_index(['timestamp', 'STID'])\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "-9a2LGUVwkxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}