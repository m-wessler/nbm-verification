{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/NBM_Reliability_New.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Import Packages & Verify Environment**"
      ],
      "metadata": {
        "id": "3Iul9JxLCbFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use PIP to install packages not already provided"
      ],
      "metadata": {
        "id": "qANfGJr5GBjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "!pip install pygrib\n",
        "!pip install swifter"
      ],
      "metadata": {
        "id": "bc2PiuEfp8VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import boto3\n",
        "import pygrib\n",
        "import swifter\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "from datetime import datetime, timedelta\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from multiprocessing import set_start_method, get_context"
      ],
      "metadata": {
        "id": "wqnsBPQOCpAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Define functions and methods**\n"
      ],
      "metadata": {
        "id": "4Ob9LRHKXAdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Variables"
      ],
      "metadata": {
        "id": "hnGcq1V6gg9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiprocess settings\n",
        "process_pool_size = cpu_count()*8\n",
        "print(f'Process Pool Size: {process_pool_size}')\n",
        "\n",
        "# Synoptic API token\n",
        "user_token = 'a2386b75ecbc4c2784db1270695dde73'\n",
        "\n",
        "# Backend APIs\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "qc_api = \"https://api.synopticdata.com/v2/stations/qcsegments?\"\n",
        "\n",
        "# Data Query APIs\n",
        "timeseries_api = \"https://api.synopticdata.com/v2/stations/timeseries?\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "\n",
        "# Assign API to element name\n",
        "synoptic_apis = {\n",
        "    'qpf':precipitation_api,\n",
        "    'maxt':statistics_api,\n",
        "    'mint':statistics_api}\n",
        "\n",
        "synoptic_networks = {\"NWS+RAWS+HADS\":\"1,2,106\",\n",
        "                     \"NWS+RAWS\":\"1,2\",\n",
        "                     \"NWS\":\"1\",\n",
        "                     \"RAWS\": \"2\",\n",
        "                     \"ALL\":\"\"}\n",
        "                    #  \"CUSTOM\": \"&network=\"+network_input,\n",
        "                    #  \"LIST\": \"&stid=\"+network_input}\n",
        "\n",
        "# Assign synoptic variable to element name\n",
        "synoptic_vars = {\n",
        "    'qpf':None,\n",
        "    'maxt':'air_temp',\n",
        "    'mint':'air_temp'}\n",
        "\n",
        "synoptic_vars_out = {\n",
        "    'qpf':'OBSERVATIONS.precipitation',\n",
        "    'maxt':'STATISTICS.air_temp_set_1.maximum',\n",
        "    'mint':'STATISTICS.air_temp_set_1.minimum',}\n",
        "\n",
        "# Assign stat type to element name\n",
        "stat_type = {\n",
        "    'qpf':'total',\n",
        "    'maxt':'maximum',\n",
        "    'mint':'minimum'}\n",
        "\n",
        "ob_hours = {\n",
        "    'qpf':['1200', '1200'],\n",
        "    'maxt':['1200', '0600'],\n",
        "    'mint':['0000', '1800']}\n",
        "\n",
        "# NBM Globals\n",
        "aws_bucket = 'noaa-nbm-grib2-pds'\n",
        "\n",
        "# Where to place the grib file (subdirs can be added in local) (not used)\n",
        "# output_dir = './'\n",
        "\n",
        "# Which grib variables do each element correlate with\n",
        "nbm_vars = {'qpf':'APCP',\n",
        "                  'maxt':'TMP',\n",
        "                  'mint':'TMP'}\n",
        "\n",
        "# Which grib levels do each element correlate with\n",
        "nbm_levs = {'qpf':'surface',\n",
        "               'maxt':'2 m above ground',\n",
        "               'mint':'2 m above ground'}\n",
        "\n",
        "# If a grib message contains any of these, exclude\n",
        "excludes = ['ens std dev', '% lev']\n",
        "\n",
        "# Fix MDL's kelvin thresholds...\n",
        "tk_fix = {233.0:233.15, 244.0:244.261, 249.0:249.817, 255.0:255.372,\n",
        "    260:260.928, 270.0:270.928, 273.0:273.15, 299.0:299.817,\n",
        "    305.0:305.372, 310.0:310.928, 316.0:316.483, 322.0:322.039}"
      ],
      "metadata": {
        "id": "ycKav9Y5gfRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Methods"
      ],
      "metadata": {
        "id": "FKuV_0blfqdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir_p(path):\n",
        "    from pathlib import Path\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def cwa_list(input_region):\n",
        "\n",
        "    input_region = input_region.upper()\n",
        "\n",
        "    region_dict ={\n",
        "        \"WR\":[\"BYZ\", \"BOI\", \"LKN\", \"EKA\", \"FGZ\", \"GGW\", \"TFX\", \"VEF\", \"LOX\", \"MFR\",\n",
        "            \"MSO\", \"PDT\", \"PSR\", \"PIH\", \"PQR\", \"REV\", \"STO\", \"SLC\", \"SGX\", \"MTR\",\n",
        "            \"HNX\", \"SEW\", \"OTX\", \"TWC\"],\n",
        "\n",
        "        \"CR\":[\"ABR\", \"BIS\", \"CYS\", \"LOT\", \"DVN\", \"BOU\", \"DMX\", \"DTX\", \"DDC\", \"DLH\",\n",
        "            \"FGF\", \"GLD\", \"GJT\", \"GRR\", \"GRB\", \"GID\", \"IND\", \"JKL\", \"EAX\", \"ARX\",\n",
        "            \"ILX\", \"LMK\", \"MQT\", \"MKX\", \"MPX\", \"LBF\", \"APX\", \"IWX\", \"OAX\", \"PAH\",\n",
        "            \"PUB\", \"UNR\", \"RIW\", \"FSD\", \"SGF\", \"LSX\", \"TOP\", \"ICT\"],\n",
        "\n",
        "        \"ER\":[\"ALY\", \"LWX\", \"BGM\", \"BOX\", \"BUF\", \"BTV\", \"CAR\", \"CTP\", \"RLX\", \"CHS\",\n",
        "            \"ILN\", \"CLE\", \"CAE\", \"GSP\", \"MHX\", \"OKX\", \"PHI\", \"PBZ\", \"GYX\", \"RAH\",\n",
        "            \"RNK\", \"AKQ\", \"ILM\"],\n",
        "\n",
        "        \"SR\":[\"ABQ\", \"AMA\", \"FFC\", \"EWX\", \"BMX\", \"BRO\", \"CRP\", \"EPZ\", \"FWD\", \"HGX\",\n",
        "            \"HUN\", \"JAN\", \"JAX\", \"KEY\", \"MRX\", \"LCH\", \"LZK\", \"LUB\", \"MLB\", \"MEG\",\n",
        "            \"MAF\", \"MFL\", \"MOB\", \"MRX\", \"OHX\", \"LIX\", \"OUN\", \"SJT\", \"SHV\", \"TAE\",\n",
        "            \"TBW\", \"TSA\"]}\n",
        "\n",
        "    if input_region == \"CONUS\":\n",
        "        return np.hstack([region_dict[region] for region in region_dict.keys()])\n",
        "    else:\n",
        "        return region_dict[input_region]"
      ],
      "metadata": {
        "id": "Uay-RJ6ufmfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synoptic API Query Methods"
      ],
      "metadata": {
        "id": "ERnc6hbxfjDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_obs_from_API(date, cwa='', output_type='csv', use_saved=True, **req):\n",
        "\n",
        "    valid = True\n",
        "    cwa_filename = req['region'] if req['region'] else cwa\n",
        "\n",
        "    output_dir = mkdir_p(f'./obs_{output_type}/')\n",
        "\n",
        "    output_file = output_dir + f'obs.{req[\"element\"]}.{req[\"ob_stat\"]}' +\\\n",
        "                    f'.{date}.{cwa_filename}.{output_type}'\n",
        "\n",
        "    if os.path.isfile(output_file) & use_saved:\n",
        "        # print(f'Output file exists for:{iter_item}')\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        json_dir = mkdir_p('./obs_json/')\n",
        "\n",
        "        json_file = json_dir + f'obs.{req[\"element\"]}.{req[\"ob_stat\"]}' +\\\n",
        "                        f'.{date}.{cwa_filename}.json'\n",
        "\n",
        "\n",
        "        adjusted_end_date = (datetime.strptime(date, '%Y%m%d') +\n",
        "                            timedelta(days=req['days_offset'])\n",
        "                            ).strftime('%Y%m%d')\n",
        "\n",
        "        if os.path.isfile(json_file) & use_saved:\n",
        "            # print(f'Polling archived JSON for: {iter_item}')\n",
        "\n",
        "            with open(json_file, 'rb+') as rfp:\n",
        "                response_dataframe = pd.json_normalize(json.load(rfp)['STATION'])\n",
        "\n",
        "        else:\n",
        "            api_query_args = {\n",
        "                'api_token':f'&token={user_token}',\n",
        "                'station_query':f'&cwa={cwa}',\n",
        "                'network_query':f'&network={req[\"network_query\"]}',\n",
        "                'start_date_query':f'&start={date}{req[\"obs_start_hour\"]}',\n",
        "                'end_date_query':f'&end={adjusted_end_date}{req[\"obs_end_hour\"]}',\n",
        "                'vars_query':(f'&pmode=totals' if req[\"element\"] == 'qpf'\n",
        "                    else f'&vars={req[\"vars_query\"]}'),\n",
        "                'stats_query':f'&type={req[\"ob_stat\"]}',\n",
        "                'timezone_query':'&obtimezone=utc',\n",
        "                'api_extras':'&fields=name,status,latitude,longitude,elevation'}\n",
        "\n",
        "            api_query = req['api'] + ''.join(\n",
        "                [api_query_args[k] for k in api_query_args.keys()])\n",
        "\n",
        "            print(f'Polling API for: {iter_item}\\n{api_query}')\n",
        "\n",
        "            status_code, response_count = None, 0\n",
        "            while (status_code != 200) & (response_count <= 10):\n",
        "                print(f'{iter_item}, HTTP:{status_code}, #:{response_count}')\n",
        "\n",
        "                # Don't sleep first try, sleep increasing amount for each retry\n",
        "                time.sleep(2*response_count)\n",
        "\n",
        "                response = requests.get(api_query)\n",
        "                # response.raise_for_status()\n",
        "\n",
        "                status_code = response.status_code\n",
        "                response_count += 1\n",
        "\n",
        "            try:\n",
        "                response_dataframe = pd.json_normalize(\n",
        "                    response.json()['STATION'])\n",
        "            except:\n",
        "                valid = False\n",
        "            else:\n",
        "                with open(json_file, 'wb+') as wfp:\n",
        "                    wfp.write(response.content)\n",
        "\n",
        "        if valid:\n",
        "            # Check ACTIVE flag (Can disable in config above if desired)\n",
        "            response_dataframe = response_dataframe[\n",
        "                response_dataframe['STATUS'] == \"ACTIVE\"]\n",
        "\n",
        "            # Un-nest the QPF totals\n",
        "            if req['element'] == 'qpf':\n",
        "                response_dataframe['TOTAL'] = [i[0]['total']\n",
        "                    for i in response_dataframe['OBSERVATIONS.precipitation']]\n",
        "\n",
        "            if output_type == 'pickle':\n",
        "            # Save out df as pickle\n",
        "                response_dataframe.to_pickle(output_file)\n",
        "\n",
        "            elif output_type == 'csv':\n",
        "            # Save out df as csv\n",
        "                response_dataframe.to_csv(output_file)\n",
        "\n",
        "            return None\n",
        "\n",
        "        else:\n",
        "            return iter_item"
      ],
      "metadata": {
        "id": "2U3d-TiqXA4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NBM Query Methods"
      ],
      "metadata": {
        "id": "-VH_D_gfqK0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ll_to_index(loclat, loclon, datalats, datalons):\n",
        "    # index, loclat, loclon = loclatlon\n",
        "    abslat = np.abs(datalats-loclat)\n",
        "    abslon = np.abs(datalons-loclon)\n",
        "    c = np.maximum(abslon, abslat)\n",
        "    latlon_idx_flat = np.argmin(c)\n",
        "    latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "    return latlon_idx\n",
        "\n",
        "def fetch_grib_from_AWS(iter_item, save_dir='./grib2/', **req):\n",
        "    from botocore import UNSIGNED\n",
        "    from botocore.client import Config\n",
        "\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    nbm_sets = ['qmd'] #, 'core']\n",
        "\n",
        "    mkdir_p(save_dir)\n",
        "\n",
        "    output_file = (save_dir +\n",
        "        f'{yyyymmdd}.t{req[\"hh\"]:02d}z.fhr{req[\"lead_time_days\"]*24:03d}.{req[\"element\"]}.grib2')\n",
        "\n",
        "    if os.path.isfile(output_file):\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        for nbm_set in nbm_sets:\n",
        "\n",
        "            bucket_dir = f'blend.{yyyymmdd}/{req[\"hh\"]:02d}/{nbm_set}/'\n",
        "\n",
        "            grib_file = f'{bucket_dir}blend.t{req[\"hh\"]:02d}z.'+\\\n",
        "                        f'{nbm_set}.f{req[\"lead_time_days\"]*24:03d}.{req[\"nbm_area\"]}.grib2'\n",
        "\n",
        "            index_file = f'{grib_file}.idx'\n",
        "\n",
        "            client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "            print(index_file)\n",
        "\n",
        "            index_data_raw = client.get_object(\n",
        "                Bucket=aws_bucket, Key=index_file)['Body'].read().decode().split('\\n')\n",
        "\n",
        "            cols = ['num', 'byte', 'date', 'var', 'level',\n",
        "                'forecast', 'fthresh', 'ftype', '']\n",
        "\n",
        "            n_data_cols = len(index_data_raw[0].split(':'))\n",
        "\n",
        "            while len(cols) > n_data_cols:\n",
        "                cols = cols[:-1]\n",
        "\n",
        "            index_data = pd.DataFrame(\n",
        "                [item.split(':') for item in index_data_raw],\n",
        "                            columns=cols)\n",
        "\n",
        "            # Clean up any ghost indicies, set the index\n",
        "            index_data = index_data[index_data['num'] != '']\n",
        "            index_data['num'] = index_data['num'].astype(int)\n",
        "            index_data = index_data.set_index('num')\n",
        "\n",
        "            # Allow byte ranging to '' (EOF)\n",
        "            index_data.loc[index_data.shape[0]+1] = ['']*index_data.shape[1]\n",
        "\n",
        "            index_subset = index_data[\n",
        "                ((index_data['var'] == req['var']) &\n",
        "                (index_data['level'] == req['level']))]\n",
        "\n",
        "            # byte start >> byte range\n",
        "            for i in index_subset.index:\n",
        "                index_subset.loc[i]['byte'] = (\n",
        "                    index_data.loc[i, 'byte'],\n",
        "                    index_data.loc[int(i)+1, 'byte'])\n",
        "\n",
        "            # Filter out excluded vars\n",
        "            for ex in excludes:\n",
        "                mask = np.column_stack([index_subset[col].str.contains(ex, na=False)\n",
        "                                        for col in index_subset])\n",
        "\n",
        "                index_subset = index_subset.loc[~mask.any(axis=1)]\n",
        "\n",
        "            # Fetch the data by byte range, write from stream\n",
        "            for index, item in index_subset.iterrows():\n",
        "                byte_range = f\"bytes={item['byte'][0]}-{item['byte'][1]}\"\n",
        "\n",
        "                output_bytes = client.get_object(\n",
        "                    Bucket=aws_bucket, Key=grib_file, Range=byte_range)\n",
        "\n",
        "                with open(output_file, 'ab') as wfp:\n",
        "                    for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                        wfp.write(chunk)\n",
        "\n",
        "    client.close()\n",
        "    return output_file\n",
        "\n",
        "def extract_nbm_value(grib_index, nbm_data):\n",
        "    return nbm_data[grib_index]"
      ],
      "metadata": {
        "id": "gAMx5QeAqK7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Set Global Variables & User Configuration**"
      ],
      "metadata": {
        "id": "ao43MTzmCz6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect user inputs\n",
        "element = 'qpf'\n",
        "element = element.lower() # Failsafe\n",
        "\n",
        "region_selection = 'WR'\n",
        "cwa_selection = 'SLC'\n",
        "\n",
        "start_date = '20231101'\n",
        "end_date = '20231115'\n",
        "\n",
        "interval_selection = 24 #6/12/24/48/72, if element==temp then False\n",
        "init_hour_selection = 12 #int(input('Desired init hour int(HH)? '))\n",
        "lead_days_selection = 1 #int(input('Desired forecast hour/lead time int(HHH)?'))\n",
        "\n",
        "# # Immediately convert user input to datetime objects\n",
        "# # start_date, end_date = [datetime.strptime(date+'0000', '%Y%m%d%H%M')\n",
        "# #     for date in [start_date, end_date]]\n",
        "\n",
        "# Convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+'0000', '%Y%m%d%H%M')\n",
        "    for date in [start_date, end_date]]"
      ],
      "metadata": {
        "id": "k9Q7q0slC6SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3a. Build arg dicts and clean up configs**\n"
      ],
      "metadata": {
        "id": "VLEMLZpWf6uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build arg dict\n",
        "synoptic_api_args = {\n",
        "    'obs_start_hour':ob_hours[element][0],\n",
        "    'obs_end_hour':ob_hours[element][1],\n",
        "    'ob_stat':stat_type[element],\n",
        "    'api':synoptic_apis[element],\n",
        "    'element':element,\n",
        "    'region':region_selection,\n",
        "    'network_query':synoptic_networks['NWS+RAWS'], # add config feature later\n",
        "    'vars_query':None if element == 'qpf'\n",
        "        else f'{synoptic_vars[element]}',\n",
        "    'days_offset':1 if element != 'mint' else 0}\n",
        "\n",
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "cwa_query = ','.join(cwa_list(region_selection)\n",
        "                    ) if region_selection is not None else cwa_selection\n",
        "\n",
        "multiprocess_function = partial(fetch_obs_from_API,\n",
        "                                cwa=cwa_query,\n",
        "                                **synoptic_api_args)"
      ],
      "metadata": {
        "id": "b8x_3t9xZPvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QPF 0/6/12/18, valid 0/6/12/18\n",
        "# MaxT 6/18 valid 6\n",
        "# MinT 6/18 valid 18\n",
        "\n",
        "# Build arg dict\n",
        "nbm_request_args = {\n",
        "    #'yyyymmdd':yyyymmdd, #input('Desired init date (YYYYMMDD)? '),\n",
        "    'interval':interval_selection,\n",
        "    'hh':init_hour_selection,\n",
        "    'lead_time_days':lead_days_selection,\n",
        "    'nbm_area':'co',\n",
        "    'element':element,\n",
        "    'var':nbm_vars[element],\n",
        "    'level':nbm_levs[element]}\n",
        "\n",
        "if ((element == 'maxt') or (element == 'mint')):\n",
        "    nbm_request_args['interval'] = False\n",
        "    nbm_request_args['hh'] = 6 if element == 'maxt' else 18\n",
        "\n",
        "# Fix offset of init time vs valid time to verify between chosen dates\n",
        "valid_hours_advance = (\n",
        "    nbm_request_args['hh'] + (nbm_request_args['lead_time_days']*24))\n",
        "\n",
        "if (valid_hours_advance) >= 24:\n",
        "    start_date -= timedelta(days=int(valid_hours_advance/24))\n",
        "    end_date -= timedelta(days=int(valid_hours_advance/24))\n",
        "\n",
        "print(start_date, end_date)"
      ],
      "metadata": {
        "id": "26AhD0ssqjLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Acquire Observations**"
      ],
      "metadata": {
        "id": "epu7XwMBC7F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multithreaded requests currently not supported by the Synoptic API\n",
        "for iter_item in date_selection_iterable:\n",
        "    multiprocess_function(iter_item)\n",
        "\n",
        "# with Pool(process_pool_size) as pool:\n",
        "#     print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "#           f'across {process_pool_size} workers')\n",
        "\n",
        "#     retry = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "#     pool.terminate()\n",
        "\n",
        "#     print('Multiprocessing Complete')\n",
        "\n",
        "# Glob together csv files\n",
        "# Need to filter by variable/region in case of region change or re-run!\n",
        "synoptic_varname = synoptic_vars_out[element]\n",
        "\n",
        "searchstring = (f'*{element}*{region_selection}*.csv'\n",
        "    if region_selection is not None else f'*{element}*{cwa_selection}*.csv')\n",
        "\n",
        "df = pd.concat(map(pd.read_csv, glob(os.path.join('./obs_csv/', searchstring))),\n",
        "               ignore_index=True)\n",
        "\n",
        "if element == 'qpf':\n",
        "    # Un-nest precipitation observations\n",
        "    df_qpf = pd.concat([pd.DataFrame(json.loads(row.replace(\"'\", '\"')))\n",
        "            for row in df[synoptic_varname]], ignore_index=True)\n",
        "\n",
        "    df = df.drop(columns=synoptic_varname).join(df_qpf)\n",
        "\n",
        "    # Rename the variable since we've changed the column name\n",
        "    synoptic_varname = 'total'\n",
        "\n",
        "# Identify the timestamp column (changes with variable)\n",
        "for k in df.keys():\n",
        "    if (('date_time' in k) or ('last_report' in k)):\n",
        "        time_col = k\n",
        "\n",
        "df.rename(columns={time_col:'timestamp'}, inplace=True)\n",
        "time_col = 'timestamp'\n",
        "\n",
        "# Convert read strings to datetime object\n",
        "df[time_col] = pd.to_datetime(df['timestamp']).round('60min')\n",
        "\n",
        "if element == 'maxt':\n",
        "    # Attribute to the day prior if UTC < 06Z otherwise attribute as stamped\n",
        "    df['timestamp'] = df['timestamp'].where(df['timestamp'].dt.hour <= 6,\n",
        "                    df['timestamp']-pd.Timedelta(1, unit='D')).dt.date\n",
        "\n",
        "elif element == 'mint':\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "\n",
        "elif element == 'qpf':\n",
        "    # Might need to do something different here so breaking into own elif...\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "\n",
        "# Drop any NaNs and sort by date with station as secondary index\n",
        "df.set_index(['timestamp'], inplace=True)\n",
        "df = df[df.index.notnull()].reset_index().set_index(['timestamp', 'STID'])\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "df = df[['LATITUDE', 'LONGITUDE', 'ELEVATION', synoptic_varname]]\n",
        "df = df.rename(columns={synoptic_varname:element.upper()})\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "iJQI-X6mN3sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Acquire NBM Data**"
      ],
      "metadata": {
        "id": "T-fIfbGvDDI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_grib_from_AWS, **nbm_request_args)\n",
        "\n",
        "# Set up this way for later additions (e.g. a 2D iterable)\n",
        "# multiprocess_iterable = [item for item in itertools.product(\n",
        "#     other_iterable, date_selection_iterable)]\n",
        "multiprocess_iterable = date_selection_iterable\n",
        "\n",
        "with get_context('fork').Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "          f'across {process_pool_size} workers')\n",
        "    grib_output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "    print('Multiprocessing Complete')"
      ],
      "metadata": {
        "id": "ScC71X2Wn-f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Calculate Statistics**"
      ],
      "metadata": {
        "id": "XfwDT94gDHGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start by matching obs with NBM values in order to streamline the bulk stats**\n"
      ],
      "metadata": {
        "id": "NQPf16p1VHGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over dates in the DataFrame, open one NBM file at a time\n",
        "for valid_date in df.index.get_level_values(0).unique():\n",
        "\n",
        "    # We are looping over the observation dates... the filenames are stamped\n",
        "    # with the INIT DATE. We need to offset the observation dates to work!\n",
        "    init_date = valid_date - pd.Timedelta(\n",
        "        nbm_request_args['lead_time_days'], 'day')\n",
        "\n",
        "    print(f'i:{init_date}, v:{valid_date}')\n",
        "\n",
        "    datestr = datetime.strftime(init_date, '%Y%m%d')\n",
        "    nbm_file = f'./grib2/{datestr}.t{nbm_request_args[\"hh\"]:02d}z' +\\\n",
        "            f'.fhr{nbm_request_args[\"lead_time_days\"]*24:03d}.{element}.grib2'\n",
        "\n",
        "    if os.path.isfile(nbm_file):\n",
        "        nbm = pygrib.open(nbm_file)\n",
        "\n",
        "        # If not yet indexed, go ahead and build the indexer\n",
        "        if 'grib_index' not in df.columns:\n",
        "\n",
        "            nbmlats, nbmlons = nbm.message(1).latlons()\n",
        "\n",
        "            df_indexed = df.reset_index()[\n",
        "                ['STID', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].drop_duplicates()\n",
        "\n",
        "            ll_to_index_mapped = partial(ll_to_index,\n",
        "                                        datalats=nbmlats, datalons=nbmlons)\n",
        "\n",
        "            print('\\nFirst pass: creating y/x grib indicies from lat/lon\\n')\n",
        "\n",
        "            df_indexed['grib_index'] = df_indexed.swifter.apply(\n",
        "                lambda x: ll_to_index_mapped(x.LATITUDE, x.LONGITUDE), axis=1)\n",
        "\n",
        "            # Extract the grid latlon\n",
        "            extract_nbm_lats_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=nbmlats)\n",
        "\n",
        "            extract_nbm_lons_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=nbmlons)\n",
        "\n",
        "            df_indexed['grib_lat'] = df_indexed['grib_index'].apply(\n",
        "                extract_nbm_lats_mapped)\n",
        "\n",
        "            df_indexed['grib_lon'] = df_indexed['grib_index'].apply(\n",
        "                extract_nbm_lons_mapped)\n",
        "\n",
        "            df_indexed.set_index('STID', inplace=True)\n",
        "\n",
        "            df = df.join(\n",
        "                df_indexed[['grib_index', 'grib_lat', 'grib_lon']]).sort_index()\n",
        "\n",
        "        # Extract the data for that date and re-insert into DataFrame\n",
        "        # Loop over each variable in the NBM file and store to DataFrame\n",
        "        # May need a placeholder column of NaNs in df for each var to make this work...\n",
        "        # Use .swifter.apply() as needed if this will speed up the process\n",
        "        # Alternatively, can use multiprocess pool to thread out the work over each date\n",
        "        # First pass this seems fast enough as it is...\n",
        "        for msg in nbm:\n",
        "\n",
        "            if 'Probability' in str(msg):\n",
        "                # print(msg)\n",
        "\n",
        "                # Deal with column names\n",
        "                if 'Precipitation' in str(msg):\n",
        "\n",
        "                    threshold_in = round(msg['upperLimit']*0.0393701, 2)\n",
        "\n",
        "                    name = f\"tp_ge_{str(threshold_in).replace('.','p')}\"\n",
        "\n",
        "                elif 'temperature' in str(msg):\n",
        "                    gtlt = 'le' if 'below' in str(msg) else 'ge'\n",
        "                    tk = (msg['lowerLimit'] if 'below'\n",
        "                            in str(msg) else msg['upperLimit'])\n",
        "                    tk = tk_fix[tk]\n",
        "                    tc = tk-273\n",
        "                    tf = (((tc)*(9/5))+32)\n",
        "                    name = f\"temp_{gtlt}_{tf:.0f}\".replace('-', 'm')\n",
        "\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                df.loc[valid_date, name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                    extract_nbm_value_mapped).values\n",
        "    else:\n",
        "        print(f'{nbm_file} not found, skipping')\n",
        "\n",
        "# Remove rows with missing data\n",
        "df = df.dropna(how='any')\n",
        "df"
      ],
      "metadata": {
        "id": "6GoPRDhCUB1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proceed to calculate the statistics from the pandas DataFrame**"
      ],
      "metadata": {
        "id": "uv1Yh2RptXP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query... Build from here\n",
        "# df.query('QPF >= 1.0')['tp_ge_1p0'].hist()"
      ],
      "metadata": {
        "id": "LTfYS0GstWtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Display Statistics**"
      ],
      "metadata": {
        "id": "Qx-I-wy2DWZc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmwq5Hj3f2qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8. Plot Visualizations**\n"
      ],
      "metadata": {
        "id": "ppm_TxePDMgN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLfJYiCwfDdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}