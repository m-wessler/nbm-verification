{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-wessler/nbm-verification/blob/main/NBM_Reliability_OBS_URMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# COLAB MARKDOWN AND USER CONFIGS                                             #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# @markdown <FONT SIZE=5>**1. Please Provide Your Synoptic API Token...**\n",
        "user_token = \"a2386b75ecbc4c2784db1270695dde73\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**2. Select Start and End Dates**\n",
        "start_date = \"2023-10-02\" # @param {type:\"date\"}\n",
        "end_date = \"2024-02-05\" # @param {type:\"date\"}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**3. For Which Element?**\n",
        "element = \"qpf\" # @param [\"maxt\", \"mint\",\"qpf\"]\n",
        "\n",
        "#temperature_threshold = -60 #@param {type:\"slider\", min:-60, max:140, step:10}\n",
        "#qpf_threshold = 0.31 #@param {type:\"slider\", min:0.01, max:5.00, step:0.01}\n",
        "\n",
        "#if element in [\"maxt\",\"mint\"]:\n",
        "#    threshold = temperature_threshold\n",
        "#elif element in [\"qpf\"]:\n",
        "#    threshold = qpf_threshold\n",
        "\n",
        "# @markdown <FONT SIZE=5>**4. For Which NBM Init Time?**\n",
        "init_hour_selection = 0 #@param {type:\"slider\", min:0, max:18, step:6}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**5. For Which Lead Time (in days)?**\n",
        "lead_days_selection = 1 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "# @markdown <FONT SIZE=5>**6. For Which Region?**\n",
        "region_selection = \"WR\" #@param [\"WR\", \"SR\", \"CR\", \"ER\", \"CONUS\",\"CWA\"]\n",
        "\n",
        "#@markdown If CWA selected, which one? (i.e. \"SLC\" for Salt Lake City)\n",
        "cwa_selection = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#6/12/24/48/72, if element==temp then False\n",
        "interval_selection = 24 if element not in ['maxt', 'mint'] else False\n",
        "\n",
        "# Not configured for user selection (yet)\n",
        "network_selection = 'NWS+RAWS'\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# INSTALL AND IMPORTS                                                         #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "!pip install boto3\n",
        "!pip install pygrib\n",
        "!pip install swifter\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import boto3\n",
        "import pygrib\n",
        "import swifter\n",
        "import requests\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from multiprocessing import set_start_method, get_context\n",
        "\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.metrics import (brier_score_loss, f1_score, log_loss,\n",
        "                                precision_score, recall_score, roc_auc_score)\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# GLOBAL VARIABLES AND GENERAL CONFIG                                         #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Multiprocess settings\n",
        "process_pool_size = 24 #cpu_count()*16\n",
        "print(f'Process Pool Size: {process_pool_size}')\n",
        "\n",
        "# Backend APIs\n",
        "metadata_api = \"https://api.synopticdata.com/v2/stations/metadata?\"\n",
        "qc_api = \"https://api.synopticdata.com/v2/stations/qcsegments?\"\n",
        "\n",
        "# Data Query APIs\n",
        "timeseries_api = \"https://api.synopticdata.com/v2/stations/timeseries?\"\n",
        "statistics_api = \"https://api.synopticlabs.org/v2/stations/statistics?\"\n",
        "precipitation_api = \"https://api.synopticdata.com/v2/stations/precipitation?\"\n",
        "\n",
        "# Assign API to element name\n",
        "synoptic_apis = {\n",
        "    'qpf':precipitation_api,\n",
        "    'maxt':statistics_api,\n",
        "    'mint':statistics_api}\n",
        "\n",
        "synoptic_networks = {\"NWS+RAWS+HADS\":\"1,2,106\",\n",
        "                     \"NWS+RAWS\":\"1,2\",\n",
        "                     \"NWS\":\"1\",\n",
        "                     \"RAWS\": \"2\",\n",
        "                     \"ALL\":\"\"}\n",
        "                    #  \"CUSTOM\": \"&network=\"+network_input,\n",
        "                    #  \"LIST\": \"&stid=\"+network_input}\n",
        "\n",
        "# Assign synoptic variable to element name\n",
        "synoptic_vars = {\n",
        "    'qpf':None,\n",
        "    'maxt':'air_temp',\n",
        "    'mint':'air_temp'}\n",
        "\n",
        "synoptic_vars_out = {\n",
        "    'qpf':'OBSERVATIONS.precipitation',\n",
        "    'maxt':'STATISTICS.air_temp_set_1.maximum',\n",
        "    'mint':'STATISTICS.air_temp_set_1.minimum',}\n",
        "\n",
        "# Assign stat type to element name\n",
        "stat_type = {\n",
        "    'qpf':'total',\n",
        "    'maxt':'maximum',\n",
        "    'mint':'minimum'}\n",
        "\n",
        "ob_hours = {\n",
        "    'qpf':['1200', '1200'],\n",
        "    'maxt':['1200', '0600'],\n",
        "    'mint':['0000', '1800']}\n",
        "\n",
        "# NBM Globals\n",
        "aws_bucket_nbm = 'noaa-nbm-grib2-pds'\n",
        "aws_bucket_urma = 'noaa-urma-pds'\n",
        "\n",
        "# Where to place the grib file (subdirs can be added in local) (not used)\n",
        "# output_dir = './'\n",
        "\n",
        "# Which grib variables do each element correlate with\n",
        "nbm_vars = {'qpf':'APCP',\n",
        "                  'maxt':'TMP',\n",
        "                  'mint':'TMP'}\n",
        "\n",
        "# Which grib levels do each element correlate with\n",
        "nbm_levs = {'qpf':'surface',\n",
        "               'maxt':'2 m above ground',\n",
        "               'mint':'2 m above ground'}\n",
        "\n",
        "# If a grib message contains any of these, exclude\n",
        "excludes = ['ens std dev', '% lev']\n",
        "\n",
        "# Fix MDL's kelvin thresholds...\n",
        "tk_fix = {233.0:233.15, 244.0:244.261, 249.0:249.817, 255.0:255.372,\n",
        "    260:260.928, 270.0:270.928, 273.0:273.15, 299.0:299.817,\n",
        "    305.0:305.372, 310.0:310.928, 316.0:316.483, 322.0:322.039}\n",
        "\n",
        "# Convert user input to datetime objects\n",
        "start_date, end_date = [datetime.strptime(date+' 0000', '%Y-%m-%d %H%M')\n",
        "    for date in [start_date, end_date]]\n",
        "\n",
        "# Build arg dict\n",
        "synoptic_api_args = {\n",
        "    'obs_start_hour':ob_hours[element][0],\n",
        "    'obs_end_hour':ob_hours[element][1],\n",
        "    'ob_stat':stat_type[element],\n",
        "    'api':synoptic_apis[element],\n",
        "    'element':element,\n",
        "    'region':region_selection,\n",
        "    'network_query':synoptic_networks[network_selection], # add config feature later\n",
        "    'vars_query':None if element == 'qpf'\n",
        "        else f'{synoptic_vars[element]}',\n",
        "    'days_offset':1 if element != 'mint' else 0}\n",
        "\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# FUNCTIONS AND METHODS (GENERAL)                                             #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def mkdir_p(path):\n",
        "    from pathlib import Path\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def cwa_list(input_region):\n",
        "\n",
        "    input_region = input_region.upper()\n",
        "\n",
        "    region_dict ={\n",
        "        \"WR\":[\"BYZ\", \"BOI\", \"LKN\", \"EKA\", \"FGZ\", \"GGW\", \"TFX\", \"VEF\", \"LOX\", \"MFR\",\n",
        "            \"MSO\", \"PDT\", \"PSR\", \"PIH\", \"PQR\", \"REV\", \"STO\", \"SLC\", \"SGX\", \"MTR\",\n",
        "            \"HNX\", \"SEW\", \"OTX\", \"TWC\"],\n",
        "\n",
        "        \"CR\":[\"ABR\", \"BIS\", \"CYS\", \"LOT\", \"DVN\", \"BOU\", \"DMX\", \"DTX\", \"DDC\", \"DLH\",\n",
        "            \"FGF\", \"GLD\", \"GJT\", \"GRR\", \"GRB\", \"GID\", \"IND\", \"JKL\", \"EAX\", \"ARX\",\n",
        "            \"ILX\", \"LMK\", \"MQT\", \"MKX\", \"MPX\", \"LBF\", \"APX\", \"IWX\", \"OAX\", \"PAH\",\n",
        "            \"PUB\", \"UNR\", \"RIW\", \"FSD\", \"SGF\", \"LSX\", \"TOP\", \"ICT\"],\n",
        "\n",
        "        \"ER\":[\"ALY\", \"LWX\", \"BGM\", \"BOX\", \"BUF\", \"BTV\", \"CAR\", \"CTP\", \"RLX\", \"CHS\",\n",
        "            \"ILN\", \"CLE\", \"CAE\", \"GSP\", \"MHX\", \"OKX\", \"PHI\", \"PBZ\", \"GYX\", \"RAH\",\n",
        "            \"RNK\", \"AKQ\", \"ILM\"],\n",
        "\n",
        "        \"SR\":[\"ABQ\", \"AMA\", \"FFC\", \"EWX\", \"BMX\", \"BRO\", \"CRP\", \"EPZ\", \"FWD\", \"HGX\",\n",
        "            \"HUN\", \"JAN\", \"JAX\", \"KEY\", \"MRX\", \"LCH\", \"LZK\", \"LUB\", \"MLB\", \"MEG\",\n",
        "            \"MAF\", \"MFL\", \"MOB\", \"MRX\", \"OHX\", \"LIX\", \"OUN\", \"SJT\", \"SHV\", \"TAE\",\n",
        "            \"TBW\", \"TSA\"]}\n",
        "\n",
        "    if input_region == \"CONUS\":\n",
        "        return np.hstack([region_dict[region] for region in region_dict.keys()])\n",
        "    else:\n",
        "        return region_dict[input_region]\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# FUNCTIONS AND METHODS (SYNOPTIC API)                                        #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def fetch_obs_from_API(date, cwa='', output_type='csv', use_saved=True, **req):\n",
        "\n",
        "    valid = True\n",
        "    cwa_filename = req['region'] if req['region'] else cwa\n",
        "\n",
        "    output_dir = mkdir_p(f'./obs_{output_type}/')\n",
        "\n",
        "    output_file = output_dir + f'obs.{req[\"element\"]}.{req[\"ob_stat\"]}' +\\\n",
        "                    f'.{date}.{cwa_filename}.{output_type}'\n",
        "\n",
        "    if os.path.isfile(output_file) & use_saved:\n",
        "        # print(f'Output file exists for:{iter_item}')\n",
        "        return output_file\n",
        "\n",
        "    else:\n",
        "        json_dir = mkdir_p('./obs_json/')\n",
        "\n",
        "        json_file = json_dir + f'obs.{req[\"element\"]}.{req[\"ob_stat\"]}' +\\\n",
        "                        f'.{date}.{cwa_filename}.json'\n",
        "\n",
        "\n",
        "        adjusted_end_date = (datetime.strptime(date, '%Y%m%d') +\n",
        "                            timedelta(days=req['days_offset'])\n",
        "                            ).strftime('%Y%m%d')\n",
        "\n",
        "        if os.path.isfile(json_file) & use_saved:\n",
        "            # print(f'Polling archived JSON for: {iter_item}')\n",
        "\n",
        "            with open(json_file, 'rb+') as rfp:\n",
        "                response_dataframe = pd.json_normalize(json.load(rfp)['STATION'])\n",
        "\n",
        "        else:\n",
        "            api_query_args = {\n",
        "                'api_token':f'&token={user_token}',\n",
        "                'station_query':f'&cwa={cwa}',\n",
        "                'network_query':f'&network={req[\"network_query\"]}',\n",
        "                'start_date_query':f'&start={date}{req[\"obs_start_hour\"]}',\n",
        "                'end_date_query':f'&end={adjusted_end_date}{req[\"obs_end_hour\"]}',\n",
        "                'vars_query':(f'&pmode=totals' if req[\"element\"] == 'qpf'\n",
        "                    else f'&vars={req[\"vars_query\"]}'),\n",
        "                'stats_query':f'&type={req[\"ob_stat\"]}',\n",
        "                'timezone_query':'&obtimezone=utc',\n",
        "                'api_extras':'&fields=name,status,latitude,longitude,elevation'+\\\n",
        "                             '&units=temp|f&complete=True'}\n",
        "\n",
        "            api_query = req['api'] + ''.join(\n",
        "                [api_query_args[k] for k in api_query_args.keys()])\n",
        "\n",
        "            print(f'Polling API for: {iter_item}\\n{api_query}')\n",
        "\n",
        "            status_code, response_count = None, 0\n",
        "            while (status_code != 200) & (response_count <= 10):\n",
        "                print(f'{iter_item}, HTTP:{status_code}, #:{response_count}')\n",
        "\n",
        "                # Don't sleep first try, sleep increasing amount for each retry\n",
        "                time.sleep(2*response_count)\n",
        "\n",
        "                response = requests.get(api_query)\n",
        "                # response.raise_for_status()\n",
        "\n",
        "                status_code = response.status_code\n",
        "                response_count += 1\n",
        "\n",
        "            try:\n",
        "                response_dataframe = pd.json_normalize(\n",
        "                    response.json()['STATION'])\n",
        "            except:\n",
        "                valid = False\n",
        "            else:\n",
        "                with open(json_file, 'wb+') as wfp:\n",
        "                    wfp.write(response.content)\n",
        "\n",
        "        if valid:\n",
        "            # Check ACTIVE flag (Can disable in config above if desired)\n",
        "            response_dataframe = response_dataframe[\n",
        "                response_dataframe['STATUS'] == \"ACTIVE\"]\n",
        "\n",
        "            # Un-nest the QPF totals\n",
        "            if req['element'] == 'qpf':\n",
        "                response_dataframe['TOTAL'] = [i[0]['total']\n",
        "                    for i in response_dataframe['OBSERVATIONS.precipitation']]\n",
        "\n",
        "            if output_type == 'pickle':\n",
        "            # Save out df as pickle\n",
        "                response_dataframe.to_pickle(output_file)\n",
        "\n",
        "            elif output_type == 'csv':\n",
        "            # Save out df as csv\n",
        "                response_dataframe.to_csv(output_file)\n",
        "\n",
        "            return None\n",
        "\n",
        "        else:\n",
        "            return iter_item\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# FUNCTIONS AND METHODS (NBM)                                                 #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "def ll_to_index(loclat, loclon, datalats, datalons):\n",
        "    # index, loclat, loclon = loclatlon\n",
        "    abslat = np.abs(datalats-loclat)\n",
        "    abslon = np.abs(datalons-loclon)\n",
        "    c = np.maximum(abslon, abslat)\n",
        "    latlon_idx_flat = np.argmin(c)\n",
        "    latlon_idx = np.unravel_index(latlon_idx_flat, datalons.shape)\n",
        "    return latlon_idx\n",
        "\n",
        "def fetch_NBMgrib_from_AWS(iter_item, save_dir='./nbm_grib2/', **req):\n",
        "    from botocore import UNSIGNED\n",
        "    from botocore.client import Config\n",
        "\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    nbm_sets = ['qmd'] #, 'core']\n",
        "\n",
        "    mkdir_p(save_dir)\n",
        "\n",
        "    output_file = (save_dir +\n",
        "        f'{yyyymmdd}.t{req[\"hh\"]:02d}z.fhr{req[\"lead_time_days\"]*24:03d}.{req[\"element\"]}.grib2')\n",
        "\n",
        "    if os.path.isfile(output_file):\n",
        "        pass\n",
        "        # return output_file\n",
        "\n",
        "    else:\n",
        "        client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "        for nbm_set in nbm_sets:\n",
        "\n",
        "            bucket_dir = f'blend.{yyyymmdd}/{req[\"hh\"]:02d}/{nbm_set}/'\n",
        "\n",
        "            grib_file = f'{bucket_dir}blend.t{req[\"hh\"]:02d}z.'+\\\n",
        "                        f'{nbm_set}.f{req[\"lead_time_days\"]*24:03d}.{req[\"nbm_area\"]}.grib2'\n",
        "\n",
        "            index_file = f'{grib_file}.idx'\n",
        "\n",
        "            index_data_raw = client.get_object(\n",
        "                Bucket=aws_bucket_nbm, Key=index_file)['Body'].read().decode().split('\\n')\n",
        "\n",
        "            cols = ['num', 'byte', 'date', 'var', 'level',\n",
        "                'forecast', 'fthresh', 'ftype', '']\n",
        "\n",
        "            n_data_cols = len(index_data_raw[0].split(':'))\n",
        "\n",
        "            while len(cols) > n_data_cols:\n",
        "                cols = cols[:-1]\n",
        "\n",
        "            index_data = pd.DataFrame(\n",
        "                [item.split(':') for item in index_data_raw],\n",
        "                            columns=cols)\n",
        "\n",
        "            # Clean up any ghost indicies, set the index\n",
        "            index_data = index_data[index_data['num'] != '']\n",
        "            index_data['num'] = index_data['num'].astype(int)\n",
        "            index_data = index_data.set_index('num')\n",
        "\n",
        "            # Allow byte ranging to '' (EOF)\n",
        "            index_data.loc[index_data.shape[0]+1] = ['']*index_data.shape[1]\n",
        "\n",
        "            index_subset = index_data[\n",
        "                ((index_data['var'] == req['var']) &\n",
        "                (index_data['level'] == req['level']))]\n",
        "\n",
        "            # byte start >> byte range\n",
        "            for i in index_subset.index:\n",
        "                index_subset.loc[i]['byte'] = (\n",
        "                    index_data.loc[i, 'byte'],\n",
        "                    index_data.loc[int(i)+1, 'byte'])\n",
        "\n",
        "            # Filter out excluded vars\n",
        "            for ex in excludes:\n",
        "                mask = np.column_stack([index_subset[col].str.contains(ex, na=False)\n",
        "                                        for col in index_subset])\n",
        "\n",
        "                index_subset = index_subset.loc[~mask.any(axis=1)]\n",
        "\n",
        "            # Fetch the data by byte range, write from stream\n",
        "            for index, item in index_subset.iterrows():\n",
        "                byte_range = f\"bytes={item['byte'][0]}-{item['byte'][1]}\"\n",
        "\n",
        "                output_bytes = client.get_object(\n",
        "                    Bucket=aws_bucket_nbm, Key=grib_file, Range=byte_range)\n",
        "\n",
        "                with open(output_file, 'ab') as wfp:\n",
        "                    for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                        wfp.write(chunk)\n",
        "\n",
        "        client.close()\n",
        "        # return output_file\n",
        "\n",
        "def fetch_URMAgrib_from_AWS(iter_item, save_dir='./urma_grib2/', **req):\n",
        "    from botocore import UNSIGNED\n",
        "    from botocore.client import Config\n",
        "\n",
        "    mkdir_p(save_dir)\n",
        "    yyyymmdd = iter_item\n",
        "\n",
        "    # For now we just need to grab the grids here... will have to deal with offset\n",
        "    # dates when calculating statistics. The iter list will need to be -/+ one day.\n",
        "    if req[\"element\"] == 'maxt':\n",
        "        hh_set = [8]\n",
        "\n",
        "    elif req[\"element\"] == 'mint':\n",
        "        hh_set = [20]\n",
        "\n",
        "    elif req[\"element\"] == 'qpf':\n",
        "        hh_set = [0, 6, 12, 18]\n",
        "\n",
        "    client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "    for hh in hh_set:\n",
        "\n",
        "        output_file = (save_dir +\n",
        "            f'urma2p5.{yyyymmdd}.t{hh:02d}z.{req[\"element\"]}.grib2')\n",
        "\n",
        "        if os.path.isfile(output_file):\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Buffering the dates can be done outside the loop when building iterable\n",
        "            bucket_dir = f'urma2p5.{yyyymmdd}/'\n",
        "\n",
        "            if element == 'qpf':\n",
        "                grib_file = f'{bucket_dir}urma2p5.{yyyymmdd}{hh:02d}.pcp_06h.wexp.grb2'\n",
        "            else:\n",
        "                grib_file = f'{bucket_dir}urma2p5.t{hh:02d}z.2dvaranl_ndfd.grb2_wexp'\n",
        "\n",
        "            output_bytes = client.get_object(Bucket=aws_bucket_urma, Key=grib_file)\n",
        "\n",
        "            with open(output_file, 'ab') as wfp:\n",
        "                for chunk in output_bytes['Body'].iter_chunks(chunk_size=4096):\n",
        "                    wfp.write(chunk)\n",
        "\n",
        "    client.close()\n",
        "\n",
        "def extract_nbm_value(grib_index, nbm_data):\n",
        "    return nbm_data[grib_index]\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# INPUT-BASED GLOBAL VARIABLES AND CONFIG                                     #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= end_date:\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "cwa_query = ','.join(cwa_list(region_selection)\n",
        "                    ) if region_selection != 'CWA' else cwa_selection\n",
        "\n",
        "multiprocess_function = partial(fetch_obs_from_API,\n",
        "                                cwa=cwa_query,\n",
        "                                **synoptic_api_args)\n",
        "\n",
        "# QPF 0/6/12/18, valid 0/6/12/18\n",
        "# MaxT 6/18 valid 6\n",
        "# MinT 6/18 valid 18\n",
        "\n",
        "# Build arg dict\n",
        "nbm_request_args = {\n",
        "    #'yyyymmdd':yyyymmdd, #input('Desired init date (YYYYMMDD)? '),\n",
        "    'interval':interval_selection,\n",
        "    'hh':init_hour_selection,\n",
        "    'lead_time_days':lead_days_selection,\n",
        "    'nbm_area':'co',\n",
        "    'element':element,\n",
        "    'var':nbm_vars[element],\n",
        "    'level':nbm_levs[element]}\n",
        "\n",
        "if ((element == 'maxt') or (element == 'mint')):\n",
        "    nbm_request_args['interval'] = False\n",
        "    nbm_request_args['hh'] = 6 if element == 'maxt' else 18\n",
        "\n",
        "# Fix offset of init time vs valid time to verify between chosen dates\n",
        "valid_hours_advance = (\n",
        "    nbm_request_args['hh'] + (nbm_request_args['lead_time_days']*24))\n",
        "\n",
        "if (valid_hours_advance) >= 24:\n",
        "    start_date -= timedelta(days=int(valid_hours_advance/24))\n",
        "    end_date -= timedelta(days=int(valid_hours_advance/24))\n",
        "\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# DATA ACQUISITION                                                            #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Multithreaded requests currently not supported by the Synoptic API\n",
        "for iter_item in date_selection_iterable:\n",
        "    multiprocess_function(iter_item)\n",
        "\n",
        "# with Pool(process_pool_size) as pool:\n",
        "#     print(f'Spooling up process pool for {len(multiprocess_iterable)} tasks '\n",
        "#           f'across {process_pool_size} workers')\n",
        "\n",
        "#     retry = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "#     pool.terminate()\n",
        "\n",
        "#     print('Multiprocessing Complete')\n",
        "\n",
        "# Glob together csv files\n",
        "# Need to filter by variable/region in case of region change or re-run!\n",
        "synoptic_varname = synoptic_vars_out[element]\n",
        "\n",
        "searchstring = (f'*{element}*{region_selection}*.csv'\n",
        "    if region_selection is not None else f'*{element}*{cwa_selection}*.csv')\n",
        "\n",
        "df = pd.concat(map(pd.read_csv, glob(os.path.join('./obs_csv/', searchstring))),\n",
        "               ignore_index=True)\n",
        "\n",
        "if element == 'qpf':\n",
        "    # Un-nest precipitation observations\n",
        "    df_qpf = pd.concat([pd.DataFrame(json.loads(row.replace(\"'\", '\"')))\n",
        "            for row in df[synoptic_varname]], ignore_index=True)\n",
        "\n",
        "    df = df.drop(columns=synoptic_varname).join(df_qpf)\n",
        "\n",
        "    # Rename the variable since we've changed the column name\n",
        "    synoptic_varname = 'total'\n",
        "\n",
        "# Identify the timestamp column (changes with variable)\n",
        "for k in df.keys():\n",
        "    if (('date_time' in k) or ('last_report' in k)):\n",
        "        time_col = k\n",
        "\n",
        "df.rename(columns={time_col:'timestamp'}, inplace=True)\n",
        "time_col = 'timestamp'\n",
        "\n",
        "# Convert read strings to datetime object\n",
        "df[time_col] = pd.to_datetime(df['timestamp']).round('60min')\n",
        "\n",
        "if element == 'maxt':\n",
        "    # Attribute to the day prior if UTC < 06Z otherwise attribute as stamped\n",
        "    df['timestamp'] = df['timestamp'].where(df['timestamp'].dt.hour <= 6,\n",
        "                    df['timestamp']-pd.Timedelta(1, unit='D')).dt.date\n",
        "\n",
        "elif element == 'mint':\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "\n",
        "elif element == 'qpf':\n",
        "    # Might need to do something different here so breaking into own elif...\n",
        "    df['timestamp'] = df['timestamp'].dt.date\n",
        "\n",
        "# Drop any NaNs and sort by date with station as secondary index\n",
        "df.set_index(['timestamp'], inplace=True)\n",
        "df = df[df.index.notnull()].reset_index().set_index(['timestamp', 'STID'])\n",
        "df.sort_index(inplace=True)\n",
        "\n",
        "if 'CWA' in df.columns:\n",
        "    df = df[['CWA', 'STATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', synoptic_varname]]\n",
        "else:\n",
        "    df = df[['LATITUDE', 'LONGITUDE', 'ELEVATION', synoptic_varname]]\n",
        "\n",
        "df = df.rename(columns={synoptic_varname:element.upper()})\n",
        "\n",
        "# Build an iterable date list from range\n",
        "iter_date = start_date\n",
        "date_selection_iterable = []\n",
        "while iter_date <= (end_date + timedelta(days=2)):\n",
        "    date_selection_iterable.append(iter_date.strftime('%Y%m%d'))\n",
        "    iter_date += timedelta(days=1)\n",
        "\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_NBMgrib_from_AWS, **nbm_request_args)\n",
        "\n",
        "# Set up this way for later additions (e.g. a 2D iterable)\n",
        "# multiprocess_iterable = [item for item in itertools.product(\n",
        "#     other_iterable, date_selection_iterable)]\n",
        "multiprocess_iterable = date_selection_iterable[:-2]\n",
        "\n",
        "with get_context('fork').Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} NBM tasks '\n",
        "          f'across {process_pool_size} workers')\n",
        "    NBMgrib_output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "    print('Multiprocessing Complete')\n",
        "\n",
        "# Gridded URMA pull for verification using NBM pull framework (AWS)\n",
        "# Assign the fixed kwargs to the function\n",
        "multiprocess_function = partial(fetch_URMAgrib_from_AWS, **nbm_request_args)\n",
        "multiprocess_iterable = date_selection_iterable\n",
        "\n",
        "with get_context('fork').Pool(process_pool_size) as pool:\n",
        "    print(f'Spooling up process pool for {len(multiprocess_iterable)} URMA tasks '\n",
        "          f'across {process_pool_size} workers')\n",
        "    URMAgrib_output_files = pool.map(multiprocess_function, multiprocess_iterable)\n",
        "    pool.terminate()\n",
        "\n",
        "    print('Multiprocessing Complete')"
      ],
      "metadata": {
        "id": "bvHqFlIRkBDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# EXTRACT DATA AND CALCULATE STATISTICS                                       #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "\n",
        "# Loop over dates in the DataFrame, open one NBM file at a time\n",
        "for valid_date in df.index.get_level_values(0).unique():\n",
        "\n",
        "    # We are looping over the observation dates... the filenames are stamped\n",
        "    # with the INIT DATE. We need to offset the observation dates to work!\n",
        "    init_date = valid_date - pd.Timedelta(\n",
        "        nbm_request_args['lead_time_days'], 'day')\n",
        "\n",
        "    print(f'extracting i:{init_date}, v:{valid_date}')\n",
        "\n",
        "    datestr = datetime.strftime(init_date, '%Y%m%d')\n",
        "    nbm_file = f'./nbm_grib2/{datestr}.t{nbm_request_args[\"hh\"]:02d}z' +\\\n",
        "            f'.fhr{nbm_request_args[\"lead_time_days\"]*24:03d}.{element}.grib2'\n",
        "\n",
        "    if os.path.isfile(nbm_file):\n",
        "        nbm = pygrib.open(nbm_file)\n",
        "\n",
        "        # If not yet indexed, go ahead and build the indexer\n",
        "        if 'grib_index' not in df.columns:\n",
        "\n",
        "            nbmlats, nbmlons = nbm.message(1).latlons()\n",
        "\n",
        "            df_indexed = df.reset_index()[\n",
        "                ['STID', 'LATITUDE', 'LONGITUDE', 'ELEVATION']].drop_duplicates()\n",
        "\n",
        "            ll_to_index_mapped = partial(ll_to_index,\n",
        "                                        datalats=nbmlats, datalons=nbmlons)\n",
        "\n",
        "            print('\\nFirst pass: creating y/x grib indicies from lat/lon\\n')\n",
        "\n",
        "            df_indexed['grib_index'] = df_indexed.swifter.apply(\n",
        "                lambda x: ll_to_index_mapped(x.LATITUDE, x.LONGITUDE), axis=1)\n",
        "\n",
        "            # Extract the grid latlon\n",
        "            extract_nbm_lats_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=nbmlats)\n",
        "\n",
        "            extract_nbm_lons_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=nbmlons)\n",
        "\n",
        "            df_indexed['grib_lat'] = df_indexed['grib_index'].apply(\n",
        "                extract_nbm_lats_mapped)\n",
        "\n",
        "            df_indexed['grib_lon'] = df_indexed['grib_index'].apply(\n",
        "                extract_nbm_lons_mapped)\n",
        "\n",
        "            df_indexed.set_index('STID', inplace=True)\n",
        "\n",
        "            df = df.join(\n",
        "                df_indexed[['grib_index', 'grib_lat', 'grib_lon']]).sort_index()\n",
        "\n",
        "        # Extract the data for that date and re-insert into DataFrame\n",
        "        # Loop over each variable in the NBM file and store to DataFrame\n",
        "        # May need a placeholder column of NaNs in df for each var to make this work...\n",
        "        # Use .swifter.apply() as needed if this will speed up the process\n",
        "        # Alternatively, can use multiprocess pool to thread out the work over each date\n",
        "        # First pass this seems fast enough as it is...\n",
        "        for msg in nbm:\n",
        "\n",
        "            if (('Probability' in str(msg)) & (('temperature' in str(msg)) or\n",
        "                ((msg.lengthOfTimeRange == interval_selection)))):\n",
        "\n",
        "                # Deal with column names\n",
        "                if (('Precipitation' in str(msg)) &\n",
        "                 (msg.lengthOfTimeRange == interval_selection)):\n",
        "\n",
        "                    threshold_in = round(msg['upperLimit']*0.0393701, 2)\n",
        "\n",
        "                    name = f\"tp_ge_{str(threshold_in).replace('.','p')}\"\n",
        "\n",
        "                elif 'temperature' in str(msg):\n",
        "                    gtlt = 'le' if 'below' in str(msg) else 'ge'\n",
        "                    tk = (msg['lowerLimit'] if 'below'\n",
        "                            in str(msg) else msg['upperLimit'])\n",
        "                    tk = tk_fix[tk]\n",
        "                    tc = tk-273\n",
        "                    tf = (((tc)*(9/5))+32)\n",
        "                    name = f\"temp_{gtlt}_{tf:.0f}\".replace('-', 'm')\n",
        "\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                df.loc[valid_date, name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                    extract_nbm_value_mapped).values\n",
        "\n",
        "            elif 'temperature at 2 metres' in str(msg): # OR precipitation clause\n",
        "                name = 'FXMAXT' if element == 'maxt' else 'FXMINT'\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                # Convert to F from K\n",
        "                df.loc[valid_date, name] = (((df.loc[valid_date]['grib_index'].apply(\n",
        "                    extract_nbm_value_mapped).values - 273.15)*(9/5))+32)\n",
        "\n",
        "            elif (('Precipitation' in str(msg)) &\n",
        "                 (msg.lengthOfTimeRange == interval_selection)):\n",
        "\n",
        "                name = 'FXQPF'\n",
        "                if name not in df.columns:\n",
        "                    df[name] = np.nan\n",
        "\n",
        "                extract_nbm_value_mapped = partial(extract_nbm_value,\n",
        "                                                nbm_data=msg.values)\n",
        "\n",
        "                df.loc[valid_date, name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                                    extract_nbm_value_mapped).values\n",
        "\n",
        "        nbm.close()\n",
        "\n",
        "        # # # # # # # # # # # # # # # # # # #\n",
        "        # Extract URMA values into dataframe\n",
        "\n",
        "        if element == 'qpf':\n",
        "\n",
        "            urma_name = 'QPF_URMA'\n",
        "            valid_hour = init_hour_selection\n",
        "\n",
        "            urma_file = f'./urma_grib2/urma2p5.{(valid_date + timedelta(days=1)).strftime(\"%Y%m%d\")}.' +\\\n",
        "                    f't{valid_hour:02d}z.{element}.grib2'\n",
        "\n",
        "            valid_datetime = pd.to_datetime(valid_date) + timedelta(hours=init_hour_selection)\n",
        "\n",
        "            # lat shape lon shape are 2d and interchangable\n",
        "            msg = np.zeros(nbmlats.shape)\n",
        "\n",
        "            for hour_offset in [18, 12, 6, 0]:\n",
        "\n",
        "                urma_datetime = valid_datetime - timedelta(hours=hour_offset)\n",
        "\n",
        "                urma_file = f'./urma_grib2/urma2p5.{(urma_datetime).strftime(\"%Y%m%d\")}.' +\\\n",
        "                        f't{urma_datetime.strftime(\"%H\")}z.{element}.grib2'\n",
        "\n",
        "                # Sum onto the initalized zero array\n",
        "                urma = pygrib.open(urma_file)\n",
        "                msg += urma.select(shortName='tp')[0].values\n",
        "                urma.close()\n",
        "\n",
        "        elif element == 'maxt':\n",
        "\n",
        "            urma_name = 'MAXT_URMA'\n",
        "            valid_hour = 8\n",
        "\n",
        "            urma_file = f'./urma_grib2/urma2p5.{(valid_date + timedelta(days=1)).strftime(\"%Y%m%d\")}.' +\\\n",
        "                    f't{valid_hour:02d}z.{element}.grib2'\n",
        "\n",
        "            urma = pygrib.open(urma_file)\n",
        "            msg = ((urma.select(shortName='tmax')[0].values - 273.15) * (9/5)) + 32\n",
        "\n",
        "        elif element == 'mint':\n",
        "\n",
        "            urma_name = 'MINT_URMA'\n",
        "            valid_hour = 20\n",
        "\n",
        "            urma_file = f'./urma_grib2/urma2p5.{(valid_date + timedelta(days=1)).strftime(\"%Y%m%d\")}.' +\\\n",
        "                    f't{valid_hour:02d}z.{element}.grib2'\n",
        "\n",
        "            urma = pygrib.open(urma_file)\n",
        "            msg = ((urma.select(shortName='tmin')[0].values - 273.15) * (9/5)) + 32\n",
        "\n",
        "        # Working\n",
        "        if urma_name not in df.columns:\n",
        "          df[urma_name] = np.nan\n",
        "\n",
        "        extract_urma_value_mapped = partial(extract_nbm_value,\n",
        "                                nbm_data=(msg))\n",
        "\n",
        "        df.loc[valid_date, urma_name] = df.loc[valid_date]['grib_index'].apply(\n",
        "                            extract_urma_value_mapped).values\n",
        "\n",
        "    else:\n",
        "        print(f'{nbm_file} not found, skipping')\n",
        "\n",
        "# Remove rows with missing data\n",
        "df = df.dropna(how='any')\n",
        "df"
      ],
      "metadata": {
        "id": "p2uG_GgjY2t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import (\n",
        "    brier_score_loss,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,)\n",
        "\n",
        "from sklearn.calibration import CalibrationDisplay\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "varname = element.upper()\n",
        "keylist = [k for k in df.columns if (('ge' in k) or ('le' in k))]\n",
        "\n",
        "threshlist = [float(\n",
        "    k.split('_')[-1].replace('p', '.').replace('m', '-')) for k in keylist]\n",
        "\n",
        "if element == 'qpf':\n",
        "    threshlist = [t*25.4 for t in threshlist]\n",
        "\n",
        "for i, t in enumerate(zip(keylist, threshlist)):\n",
        "\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(6, 7))\n",
        "    gs = fig.add_gridspec(nrows=4, ncols=3, left=0.05, right=0.5, wspace=0.05)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[:-1, :])\n",
        "    ax2 = fig.add_subplot(gs[-1, :-1])\n",
        "    ax3 = fig.add_subplot(gs[-1, -1])\n",
        "\n",
        "    thresh_text, thresh = t\n",
        "\n",
        "    if 'ge' in thresh_text:\n",
        "        y_test_ob = np.where(df[varname] >=  thresh, 1, 0)\n",
        "        y_test = np.where(df[f'{varname}_URMA'] >= thresh, 1, 0)\n",
        "\n",
        "    elif 'le' in thresh_text:\n",
        "        y_test_ob = np.where(df[varname] <=  thresh, 1, 0)\n",
        "        y_test = np.where(df[f'{varname}_URMA'] <= thresh, 1, 0)\n",
        "\n",
        "    y_prob = df[thresh_text]/100\n",
        "    y_pred = np.where(df['FX'+varname] >= thresh, 1, 0)\n",
        "\n",
        "    # Calibration Curves/Reliability Diagrams\n",
        "    CalibrationDisplay.from_predictions(y_test, y_prob, n_bins=10, ax=ax1, name='URMA', ref_line=False, color='blue')\n",
        "    CalibrationDisplay.from_predictions(y_test_ob, y_prob, n_bins=10, ax=ax1, name='OBS', ref_line=False, color='red')\n",
        "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.6, zorder=-1)\n",
        "\n",
        "    ax1.set_xlabel('Forecast Probability')\n",
        "    ax1.set_ylabel('Observed Frequency')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Sharpness Diagram\n",
        "    ax2.hist(y_prob, bins=10, density=True, rwidth=0.8, log=True, color='k')\n",
        "    ax2.set_xlabel('Forecast Probability')\n",
        "    ax2.set_ylabel('Relative Frequency')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # ROC-AUC Curves\n",
        "    RocCurveDisplay.from_predictions(y_test, y_prob, ax=ax3, name='URMA', color='blue')\n",
        "    RocCurveDisplay.from_predictions(y_test_ob, y_prob, ax=ax3, name='OBS', color='red')\n",
        "\n",
        "    ax3.set_xlabel('False Positive Rate')\n",
        "    ax3.set_ylabel('True Positive Rate')\n",
        "    ax3.grid(True)\n",
        "    ax3.get_legend().remove()\n",
        "\n",
        "    # Skill Scores\n",
        "    scores = defaultdict(list)\n",
        "    for name in ['OBS', 'URMA']:\n",
        "\n",
        "        _y_test = y_test if name == 'URMA' else y_test_ob\n",
        "\n",
        "        scores[\"Classifier\"].append(name)\n",
        "\n",
        "        for metric in [brier_score_loss, log_loss, roc_auc_score]:\n",
        "            try:\n",
        "                score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
        "                scores[score_name].append(metric(_y_test, y_prob))\n",
        "            except:\n",
        "                scores[score_name].append(np.nan)\n",
        "\n",
        "        for metric in [precision_score, recall_score, f1_score]:\n",
        "            score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n",
        "            scores[score_name].append(metric(_y_test, y_pred))\n",
        "\n",
        "    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n",
        "    score_df.round(decimals=3)\n",
        "\n",
        "    score_df.rename(columns={'Brier  loss':'Brier Score',\n",
        "                            'Log loss':'Log Loss',\n",
        "                            'Roc auc ':'ROC AUC'}, inplace=True)\n",
        "\n",
        "    ax1.table(cellText=score_df.values.round(3),\n",
        "        colWidths=[0.25]*len(score_df.columns),\n",
        "        rowLabels=score_df.index,\n",
        "        colLabels=score_df.columns,\n",
        "        cellLoc='center', rowLoc='center',\n",
        "        loc='bottom', bbox=[0., -1.075, 1., 0.25])\n",
        "\n",
        "    # Title/Labels\n",
        "    n_urma = y_test.sum()\n",
        "    n_ob = y_test_ob.sum()\n",
        "    n_sites = df.index.get_level_values(1).unique().size\n",
        "\n",
        "    suptitle = (f'{region_selection} n_stations ({network_selection}): {n_sites}\\n'+\\\n",
        "                f'{start_date} - {end_date}')\n",
        "\n",
        "    ax1.set_title(suptitle + '\\n'*2 +\\\n",
        "            f'{varname} {thresh_text}\\nn_events (URMA/OBS): {n_urma}/{n_ob}')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "86wGLSjRVphl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}